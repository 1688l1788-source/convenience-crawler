import os
import re
import time
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

try:
    from supabase import create_client, Client
except ImportError:
    from supabase import create_client

class CUCrawler:
    def __init__(self):
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_SERVICE_KEY')
        
        if not supabase_url or not supabase_key:
            raise Exception("Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            self.supabase = create_client(supabase_url, supabase_key)
        except TypeError:
            self.supabase = create_client(
                supabase_url=supabase_url,
                supabase_key=supabase_key
            )
        
        self.brand_id = 1
        self.base_url = "https://cu.bgfretail.com"
        
        self.category_urls = [
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=1", "ê°„í¸ì‹ì‚¬"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=2", "ê³¼ìë¥˜"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=3", "ì•„ì´ìŠ¤í¬ë¦¼"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=4", "ì‹í’ˆ"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=5", "ìŒë£Œ"),
        ]
    
    def setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    
    def crawl_category(self, driver, category_url, category_name):
        print(f"\nğŸ“‚ {category_name} í¬ë¡¤ë§ ì¤‘...")
        products = []
        
        try:
            driver.get(category_url)
            time.sleep(5)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸° - ë” ì •í™•í•œ ì„ íƒì ì‚¬ìš©
            product_items = soup.select('div.prodListWrap ul.prodList li')
            
            if not product_items:
                product_items = soup.select('ul.prodList > li')
            
            if not product_items:
                product_items = soup.select('div[class*="prod"] li')
            
            print(f"  ğŸ” {len(product_items)}ê°œ ì•„ì´í…œ ë°œê²¬")
            
            for idx, item in enumerate(product_items[:30]):
                try:
                    # 1. ë§í¬ ë¨¼ì € ì¶”ì¶œ (ê°€ì¥ ì¤‘ìš”)
                    link_tag = item.find('a')
                    if not link_tag or not link_tag.get('href'):
                        continue
                    
                    href = link_tag.get('href')
                    
                    # javascript: ë§í¬ëŠ” ë¬´ì‹œ (ìƒì„¸ í˜ì´ì§€ ì•„ë‹˜)
                    if 'javascript' in href or href == '#':
                        continue
                    
                    # ê°œë³„ ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                    if href.startswith('http'):
                        source_url = href
                    elif href.startswith('/'):
                        source_url = self.base_url + href
                    else:
                        source_url = self.base_url + '/' + href
                    
                    # ìƒì„¸í˜ì´ì§€ ë§í¬ê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
                    if 'goodsDetail' not in source_url and 'itemId' not in source_url:
                        continue
                    
                    # 2. ì´ë¯¸ì§€ ì¶”ì¶œ
                    img_tag = item.find('img')
                    if not img_tag:
                        continue
                        
                    image_url = img_tag.get('src')
                    if not image_url or 'blank' in image_url or 'noimage' in image_url:
                        continue
                        
                    if not image_url.startswith('http'):
                        if image_url.startswith('//'):
                            image_url = 'https:' + image_url
                        else:
                            image_url = self.base_url + image_url
                    
                    # 3. ì œëª© ì¶”ì¶œ
                    title = img_tag.get('alt')
                    
                    if not title or len(title) < 2:
                        # altê°€ ì—†ìœ¼ë©´ name í´ë˜ìŠ¤ì—ì„œ ì°¾ê¸°
                        name_tag = item.select_one('.name, .prodName, .prod_name, p.name')
                        if name_tag:
                            title = name_tag.get_text(strip=True)
                    
                    if not title or len(title) < 2:
                        continue
                    
                    # íŒŒì¼ëª…ì´ ì œëª©ì¸ ê²½ìš° ìŠ¤í‚µ (ì‹¤ì œ ìƒí’ˆëª…ì´ ì•„ë‹˜)
                    if title.endswith('.jpg') or title.endswith('.png'):
                        # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„
                        try:
                            driver.get(source_url)
                            time.sleep(2)
                            detail_soup = BeautifulSoup(driver.page_source, 'html.parser')
                            title_elem = detail_soup.select_one('.prodTitle, .prod_tit, h3, .title')
                            if title_elem:
                                title = title_elem.get_text(strip=True)
                            driver.back()
                            time.sleep(1)
                        except:
                            continue
                    
                    # ì—¬ì „íˆ íŒŒì¼ëª…ì´ë©´ ìŠ¤í‚µ
                    if title.endswith('.jpg') or title.endswith('.png'):
                        continue

                    # 4. ê°€ê²© ì¶”ì¶œ
                    price = 0
                    price_tag = item.select_one('.price, .val, span.val, .cost')
                    
                    if price_tag:
                        price_text = price_tag.get_text()
                        numbers = re.findall(r'\d+', price_text.replace(',', ''))
                        if numbers:
                            price = max([int(n) for n in numbers if 100 <= int(n) < 1000000])
                    
                    # ê°€ê²©ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    if price == 0:
                        text = item.get_text()
                        numbers = re.findall(r'\d+', text.replace(',', ''))
                        if numbers:
                            # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„ë§Œ ì„ íƒ (100ì› ~ 100ë§Œì›)
                            valid_prices = [int(n) for n in numbers if 100 <= int(n) < 1000000]
                            if valid_prices:
                                price = max(valid_prices)
                    
                    # ê²°ê³¼ ì¶”ê°€
                    product = {
                        'brand_id': self.brand_id,
                        'title': title,
                        'normalized_title': self.normalize_title(title),
                        'price': price,
                        'category': category_name,
                        'launch_date': datetime.now().date().isoformat(),
                        'image_url': image_url,
                        'source_url': source_url,
                        'is_active': True
                    }
                    
                    products.append(product)
                    print(f"    âœ“ {title[:30]} ({price}ì›)")
                    
                except Exception as e:
                    continue
            
        except Exception as e:
            print(f"  âŒ {category_name} ì˜¤ë¥˜: {e}")
        
        return products
    
    def crawl(self):
        print("ğŸª CU ì‹ ì œí’ˆ í¬ë¡¤ë§ ì‹œì‘...")
        driver = None
        all_products = []
        
        try:
            driver = self.setup_driver()
            
            for url, name in self.category_urls:
                products = self.crawl_category(driver, url, name)
                all_products.extend(products)
                time.sleep(3)
            
            return all_products
            
        except Exception as e:
            print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {e}")
            return []
            
        finally:
            if driver:
                driver.quit()
    
    def normalize_title(self, title):
        normalized = re.sub(r'\s+', ' ', title)
        normalized = re.sub(r'[^\w\sê°€-í£]', '', normalized)
        return normalized.strip().upper()
    
    def save_to_db(self, products):
        if not products:
            return 0
        
        print(f"\nğŸ’¾ DB ì €ì¥ ì‹œì‘... ({len(products)}ê°œ)")
        try:
            # ì¤‘ë³µ ì œê±° (ê°™ì€ normalized_title)
            seen = set()
            unique_products = []
            for p in products:
                key = p['normalized_title'] + p['category']
                if key not in seen:
                    seen.add(key)
                    unique_products.append(p)
            
            print(f"  ğŸ“¦ ì¤‘ë³µ ì œê±° í›„: {len(unique_products)}ê°œ")
            
            # ë°°ì¹˜ë¡œ ì €ì¥
            self.supabase.table('new_products').upsert(
                unique_products, 
                on_conflict='normalized_title,launch_date'
            ).execute()
            
            print(f"âœ… {len(unique_products)}ê°œ ì €ì¥ ì™„ë£Œ!")
            return len(unique_products)
                
        except Exception as e:
            print(f"âš ï¸ Batch ì €ì¥ ì‹¤íŒ¨, ê°œë³„ ì €ì¥ ì‹œë„...")
            success_count = 0
            for p in unique_products:
                try:
                    self.supabase.table('new_products').insert(p).execute()
                    success_count += 1
                except:
                    pass
            print(f"âœ… {success_count}ê°œ ì €ì¥ ì™„ë£Œ (ê°œë³„)")
            return success_count

def main():
    try:
        crawler = CUCrawler()
        products = crawler.crawl()
        crawler.save_to_db(products)
    except Exception as e:
        print(e)
        exit(1)

if __name__ == "__main__":
    main()
