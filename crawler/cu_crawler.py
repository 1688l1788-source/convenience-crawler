import os
import re
import time
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

try:
    from supabase import create_client, Client
except ImportError:
    from supabase import create_client

class CUCrawler:
    def __init__(self):
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_SERVICE_KEY')
        
        if not supabase_url or not supabase_key:
            raise Exception("Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            self.supabase = create_client(supabase_url, supabase_key)
        except TypeError:
            self.supabase = create_client(
                supabase_url=supabase_url,
                supabase_key=supabase_key
            )
        
        self.brand_id = 1
        self.base_url = "https://cu.bgfretail.com"
        
        self.category_urls = [
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=1", "ê°„í¸ì‹ì‚¬"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=2", "ê³¼ìë¥˜"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=3", "ì•„ì´ìŠ¤í¬ë¦¼"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=4", "ì‹í’ˆ"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=5", "ìŒë£Œ"),
        ]
    
    def setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    
    def extract_gdidx(self, item):
        """onclick=view(26285) ì—ì„œ gdIdx ì¶”ì¶œ"""
        try:
            # onclick ì†ì„± ì°¾ê¸°
            clickable = item.find(attrs={'onclick': True})
            if clickable:
                onclick = clickable.get('onclick', '')
                # view(26285) íŒ¨í„´ì—ì„œ ìˆ«ì ì¶”ì¶œ
                match = re.search(r'view\((\d+)\)', onclick)
                if match:
                    return match.group(1)
        except:
            pass
        return None
    
    def crawl_category(self, driver, category_url, category_name):
        print(f"\nğŸ“‚ {category_name} í¬ë¡¤ë§ ì¤‘...")
        products = []
        
        try:
            driver.get(category_url)
            time.sleep(5)
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # ì œí’ˆ ì˜ì—­ ì°¾ê¸°
            product_area = soup.select_one('.prodListWrap, .prodArea')
            if not product_area:
                print(f"  âš ï¸ ì œí’ˆ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return products
            
            # ì œí’ˆ í•­ëª© ì°¾ê¸°
            items = product_area.select('li')
            print(f"  ğŸ” {len(items)}ê°œ í•­ëª© ë°œê²¬")
            
            for item in items:
                try:
                    # 1. ì œí’ˆ ì´ë¯¸ì§€ í™•ì¸
                    img = item.select_one('img[src*="/product/"]')
                    if not img:
                        continue
                    
                    image_url = img.get('src', '')
                    
                    # New íƒœê·¸ë‚˜ ì•„ì´ì½˜ ì œì™¸
                    if 'icon' in image_url or 'tag_' in image_url or 'blank' in image_url:
                        continue
                    
                    # HTTPS ë³€í™˜
                    if image_url.startswith('//'):
                        image_url = 'https:' + image_url
                    elif not image_url.startswith('http'):
                        image_url = self.base_url + image_url
                    
                    # 2. ì œí’ˆëª… ì¶”ì¶œ
                    name_tag = item.select_one('p')
                    if not name_tag:
                        continue
                    
                    title = name_tag.get_text(strip=True)
                    
                    # ì œí’ˆëª… ê²€ì¦
                    if not title or len(title) < 2:
                        continue
                    
                    # íŒŒì¼ëª…ì´ë‚˜ New ì œì™¸
                    if title.endswith('.jpg') or title.endswith('.png') or title == 'New':
                        continue
                    
                    # 3. gdIdx ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ URL)
                    gdidx = self.extract_gdidx(item)
                    
                    if gdidx:
                        source_url = f"{self.base_url}/product/view.do?gdIdx={gdidx}&category=product"
                    else:
                        source_url = category_url  # gdIdx ì—†ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€
                    
                    # 4. ê°€ê²© ì¶”ì¶œ
                    price = 0
                    price_tag = item.select_one('.price, .val, span[class*="price"]')
                    
                    if price_tag:
                        price_text = price_tag.get_text()
                        numbers = re.findall(r'\d+', price_text.replace(',', ''))
                        if numbers:
                            valid = [int(n) for n in numbers if 100 <= int(n) < 1000000]
                            if valid:
                                price = max(valid)
                    
                    # ê°€ê²© ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ
                    if price == 0:
                        all_text = item.get_text()
                        matches = re.findall(r'(\d{1,3}(?:,\d{3})*|\d+)\s*ì›', all_text)
                        if matches:
                            price = int(matches[0].replace(',', ''))
                    
                    # 5. ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ê²€ì¦
                    if not self.validate_category(title, category_name):
                        print(f"    âš ï¸ ì¹´í…Œê³ ë¦¬ ë¶ˆì¼ì¹˜ ìŠ¤í‚µ: {title}")
                        continue
                    
                    # ê²°ê³¼ ì¶”ê°€
                    product = {
                        'brand_id': self.brand_id,
                        'title': title,
                        'normalized_title': self.normalize_title(title),
                        'price': price,
                        'category': category_name,
                        'launch_date': datetime.now().date().isoformat(),
                        'image_url': image_url,
                        'source_url': source_url,
                        'is_active': True
                    }
                    
                    products.append(product)
                    print(f"    âœ“ {title} ({price}ì›) [{gdidx or 'N/A'}]")
                    
                except Exception as e:
                    continue
            
            print(f"  âœ… {len(products)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âŒ {category_name} ì˜¤ë¥˜: {e}")
        
        return products
    
    def validate_category(self, title, category):
        """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œë¡œ ê²€ì¦"""
        title_lower = title.lower()
        
        keywords = {
            'ì•„ì´ìŠ¤í¬ë¦¼': ['ì•„ì´ìŠ¤í¬ë¦¼', 'ë¹™ê³¼', 'ì½˜', 'ë°”', 'ìŠ¬ëŸ¬ì‹œ', 'ì•„ì´ìŠ¤', 'ice', 'ì†Œí”„íŠ¸', 'ì ¤ë¼ë˜', 'ì…”ë²—', 'ì†Œë¥´ë² ', 'íŒì½˜'],
            'ê³¼ìë¥˜': ['ê³¼ì', 'ìŠ¤ë‚µ', 'ì¹©', 'ì¿ í‚¤', 'ë¹„ìŠ¤í‚·', 'ì´ˆì½œë¦¿', 'ì‚¬íƒ•', 'ì ¤ë¦¬', 'ê»Œ', 'ìº”ë””', 'ì›¨í•˜ìŠ¤', 'í¬ë˜ì»¤'],
            'ìŒë£Œ': ['ìŒë£Œ', 'ì£¼ìŠ¤', 'ì»¤í”¼', 'ì°¨', 'ì›Œí„°', 'íƒ„ì‚°', 'ì—ë„ˆì§€', 'ì´ì˜¨', 'ë°€í¬', 'ë¼ë–¼', 'ì—ì´ë“œ', 'ìŠ¤ë¬´ë””'],
            'ê°„í¸ì‹ì‚¬': ['ë„ì‹œë½', 'ê¹€ë°¥', 'ìƒŒë“œìœ„ì¹˜', 'ì‚¼ê°', 'ì£¼ë¨¹ë°¥', 'í–„ë²„ê±°', 'í•«ë„ê·¸', 'í† ìŠ¤íŠ¸', 'ë¡¤', 'ë©'],
            'ì‹í’ˆ': ['ë¼ë©´', 'ì»µë¼ë©´', 'ìš°ìœ ', 'ë¹µ', 'ê³„ë€', 'ì¹˜ì¦ˆ', 'í–„', 'ì†Œì‹œì§€', 'ë‘ë¶€', 'ê¹€', 'ëƒ‰ë™']
        }
        
        if category in keywords:
            return any(keyword in title_lower for keyword in keywords[category])
        
        return True
    
    def crawl(self):
        print("ğŸª CU ì‹ ì œí’ˆ í¬ë¡¤ë§ ì‹œì‘...")
        driver = None
        all_products = []
        
        try:
            driver = self.setup_driver()
            
            for url, name in self.category_urls:
                products = self.crawl_category(driver, url, name)
                all_products.extend(products)
                time.sleep(3)
            
            return all_products
            
        except Exception as e:
            print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {e}")
            return []
            
        finally:
            if driver:
                driver.quit()
    
    def normalize_title(self, title):
        normalized = re.sub(r'\s+', ' ', title)
        normalized = re.sub(r'[^\w\sê°€-í£]', '', normalized)
        return normalized.strip().upper()
    
    def save_to_db(self, products):
        if not products:
            print("âš ï¸ ì €ì¥í•  ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤")
            return 0
        
        print(f"\nğŸ’¾ DB ì €ì¥ ì‹œì‘... ({len(products)}ê°œ)")
        
        try:
            # ì¤‘ë³µ ì œê±°
            seen = set()
            unique_products = []
            for p in products:
                key = f"{p['normalized_title']}_{p['category']}"
                if key not in seen:
                    seen.add(key)
                    unique_products.append(p)
            
            print(f"  ğŸ“¦ ì¤‘ë³µ ì œê±° í›„: {len(unique_products)}ê°œ")
            
            # ë°°ì¹˜ ì €ì¥
            self.supabase.table('new_products').upsert(
                unique_products,
                on_conflict='normalized_title,launch_date'
            ).execute()
            
            print(f"âœ… {len(unique_products)}ê°œ ì €ì¥ ì™„ë£Œ!")
            return len(unique_products)
            
        except Exception as e:
            print(f"âš ï¸ Batch ì €ì¥ ì‹¤íŒ¨: {e}")
            print(f"  ê°œë³„ ì €ì¥ ì‹œë„...")
            
            success_count = 0
            for p in unique_products:
                try:
                    self.supabase.table('new_products').insert(p).execute()
                    success_count += 1
                except:
                    pass
            
            print(f"âœ… {success_count}ê°œ ì €ì¥ ì™„ë£Œ (ê°œë³„)")
            return success_count

def main():
    try:
        crawler = CUCrawler()
        products = crawler.crawl()
        
        if products:
            crawler.save_to_db(products)
        else:
            print("âŒ ìˆ˜ì§‘ëœ ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤")
            exit(1)
            
    except Exception as e:
        print(f"âŒ í¬ë¡¤ëŸ¬ ì˜¤ë¥˜: {e}")
        exit(1)

if __name__ == "__main__":
    main()
