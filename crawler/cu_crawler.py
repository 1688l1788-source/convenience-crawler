import os
import re
import time
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

try:
    from supabase import create_client, Client
except ImportError:
    from supabase import create_client

class CUCrawler:
    def __init__(self):
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_SERVICE_KEY')
        
        if not supabase_url or not supabase_key:
            raise Exception("Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            self.supabase = create_client(supabase_url, supabase_key)
        except TypeError:
            self.supabase = create_client(
                supabase_url=supabase_url,
                supabase_key=supabase_key
            )
        
        self.brand_id = 1
    
    def setup_driver(self):
        """Selenium í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    
    def crawl(self):
        print("ðŸª CU í¬ë¡¤ë§ ì‹œìž‘...")
        driver = None
        
        try:
            driver = self.setup_driver()
            url = "https://cu.bgfretail.com/product/pb.do"
            
            print(f"\nðŸ” ì ‘ì†: {url}")
            driver.get(url)
            
            # íŽ˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            time.sleep(3)
            
            # ì œí’ˆ ëª©ë¡ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "ul.prod_list li, .prodListWrap li"))
                )
            except:
                print("âš ï¸ ì œí’ˆ ëª©ë¡ ë¡œë”© íƒ€ìž„ì•„ì›ƒ")
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # ì œí’ˆ ì°¾ê¸°
            products_html = soup.select('ul.prod_list > li')
            if not products_html:
                products_html = soup.select('.prodListWrap li')
            if not products_html:
                products_html = soup.select('li[class*="prod"]')
            
            print(f"ðŸ“¦ ë°œê²¬: {len(products_html)}ê°œ")
            
            products = []
            
            for idx, item in enumerate(products_html[:20]):
                try:
                    # ì œí’ˆëª…
                    name_elem = item.select_one('.prodName, .prod_name, strong, h3')
                    if not name_elem:
                        continue
                    
                    title = name_elem.text.strip()
                    if not title or len(title) < 3 or 'ì›' in title[:5]:
                        continue
                    
                    # ê°€ê²©
                    price = None
                    price_elem = item.select_one('.price, .prodPrice, dd')
                    if price_elem:
                        price_text = price_elem.text.strip()
                        numbers = re.findall(r'\d+', price_text.replace(',', ''))
                        if numbers:
                            price = int(''.join(numbers))
                    
                    # ì´ë¯¸ì§€
                    img = item.select_one('img')
                    image_url = None
                    if img:
                        image_url = img.get('src') or img.get('data-src')
                        if image_url:
                            if image_url.startswith('//'):
                                image_url = 'https:' + image_url
                            elif not image_url.startswith('http'):
                                image_url = 'https://cu.bgfretail.com' + image_url
                    
                    # ë§í¬
                    link = item.select_one('a')
                    source_url = url
                    if link and link.get('href'):
                        href = link.get('href')
                        if href.startswith('http'):
                            source_url = href
                        elif href.startswith('/'):
                            source_url = 'https://cu.bgfretail.com' + href
                    
                    product = {
                        'brand_id': self.brand_id,
                        'title': title,
                        'normalized_title': self.normalize_title(title),
                        'price': price,
                        'category': self.categorize(title),
                        'launch_date': datetime.now().date().isoformat(),
                        'image_url': image_url,
                        'source_url': source_url,
                        'is_active': True
                    }
                    
                    products.append(product)
                    print(f"  âœ“ {idx+1}. {title[:40]}")
                    
                except Exception as e:
                    print(f"  âœ— íŒŒì‹± ì˜¤ë¥˜: {e}")
            
            return products
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
            
        finally:
            if driver:
                driver.quit()
    
    def normalize_title(self, title):
        normalized = re.sub(r'\s+', ' ', title)
        normalized = re.sub(r'[^\w\sê°€-íž£]', '', normalized)
        return normalized.strip().upper()
    
    def categorize(self, title):
        title_lower = title.lower()
        keywords = {
            'ìŒë£Œ': ['ìŒë£Œ', 'ì£¼ìŠ¤', 'ì»¤í”¼', 'ìš°ìœ ', 'ì°¨', 'ì›Œí„°', 'ì‚¬ì´ë‹¤', 'ì½œë¼'],
            'ê³¼ìž': ['ê³¼ìž', 'ì´ˆì½œë¦¿', 'ì‚¬íƒ•', 'ì ¤ë¦¬', 'ì¿ í‚¤', 'ë¹„ìŠ¤í‚·', 'ìŠ¤ë‚µ'],
            'ì¦‰ì„ì‹í’ˆ': ['ë„ì‹œë½', 'ê¹€ë°¥', 'ìƒŒë“œìœ„ì¹˜', 'ì‚¼ê°ê¹€ë°¥', 'í•«ë„ê·¸', 'í–„ë²„ê±°'],
            'ë¼ë©´': ['ë¼ë©´', 'ì»µë¼ë©´', 'ì§œíŒŒê²Œí‹°', 'ì§œìž¥ë©´'],
            'ì•„ì´ìŠ¤í¬ë¦¼': ['ì•„ì´ìŠ¤í¬ë¦¼', 'ë¹™ê³¼', 'ì•„ì´ìŠ¤ë°”', 'ì½˜']
        }
        for category, words in keywords.items():
            if any(word in title_lower for word in words):
                return category
        return 'ê¸°íƒ€'
    
    def save_to_db(self, products):
        if not products:
            print("ðŸ’¤ ì €ìž¥í•  ì œí’ˆ ì—†ìŒ")
            return 0
        
        print(f"\nðŸ’¾ DB ì €ìž¥ ì‹œìž‘... ({len(products)}ê°œ)")
        
        try:
            # ì¤‘ë³µ ì²´í¬
            thirty_days_ago = (datetime.now() - timedelta(days=30)).date().isoformat()
            existing = self.supabase.table('new_products')\
                .select('normalized_title, launch_date')\
                .eq('brand_id', self.brand_id)\
                .gte('launch_date', thirty_days_ago)\
                .execute()
            
            existing_keys = {
                f"{p['normalized_title']}_{p['launch_date']}" 
                for p in existing.data
            }
            
            new_products = [
                p for p in products 
                if f"{p['normalized_title']}_{p['launch_date']}" not in existing_keys
            ]
            
            if new_products:
                self.supabase.table('new_products').insert(new_products).execute()
                print(f"âœ… {len(new_products)}ê°œ ì €ìž¥ ì™„ë£Œ!")
                for p in new_products[:5]:
                    print(f"  - {p['title'][:40]}")
                return len(new_products)
            else:
                print("â„¹ï¸ ëª¨ë‘ ê¸°ì¡´ ì œí’ˆ (ì‹ ê·œ ì—†ìŒ)")
                return 0
                
        except Exception as e:
            print(f"âŒ DB ì €ìž¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return 0

def main():
    print("="*60)
    print("ðŸª CU ì‹ ì œí’ˆ í¬ë¡¤ëŸ¬ (Selenium)")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    try:
        crawler = CUCrawler()
        products = crawler.crawl()
        new_count = crawler.save_to_db(products)
        
        print("\n" + "="*60)
        print(f"âœ¨ ì™„ë£Œ!")
        print(f"ðŸ“Š ìˆ˜ì§‘: {len(products)}ê°œ | ì‹ ê·œ: {new_count}ê°œ")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
