import os
import re
import time
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

try:
    from supabase import create_client, Client
except ImportError:
    from supabase import create_client

class CUCrawler:
    def __init__(self):
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_SERVICE_KEY')
        
        if not supabase_url or not supabase_key:
            raise Exception("Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            self.supabase = create_client(supabase_url, supabase_key)
        except TypeError:
            self.supabase = create_client(
                supabase_url=supabase_url,
                supabase_key=supabase_key
            )
        
        self.brand_id = 1
        self.base_url = "https://cu.bgfretail.com"
        
        self.category_urls = [
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=1", "ê°„í¸ì‹ì‚¬"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=2", "ê³¼ìë¥˜"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=3", "ì•„ì´ìŠ¤í¬ë¦¼"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=4", "ì‹í’ˆ"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=5", "ìŒë£Œ"),
        ]
    
    def setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    
    def crawl_category(self, driver, category_url, category_name):
        print(f"\nğŸ“‚ {category_name} í¬ë¡¤ë§ ì¤‘...")
        products = []
        
        try:
            driver.get(category_url)
            time.sleep(5)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # 1. ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸° (ê°€ì¥ í™•ì‹¤í•œ ì„ íƒì ì‚¬ìš©)
            product_items = soup.select('div[class*="prod"] li')
            if not product_items:
                product_items = soup.select('ul li') # fallback
            
            print(f"  ğŸ” {len(product_items)}ê°œ ì•„ì´í…œ ë°œê²¬ (ìœ íš¨ì„± ê²€ì‚¬ ì „)")
            
            for idx, item in enumerate(product_items[:30]):
                try:
                    # 2. ì´ë¯¸ì§€ ì¶”ì¶œ (ê°€ì¥ ì¤‘ìš”)
                    img_tag = item.find('img')
                    if not img_tag:
                        continue
                        
                    image_url = img_tag.get('src')
                    if not image_url or 'blank' in image_url:
                        continue
                        
                    if not image_url.startswith('http'):
                        if image_url.startswith('//'):
                            image_url = 'https:' + image_url
                        else:
                            image_url = self.base_url + image_url
                            
                    title = img_tag.get('alt')
                    
                    # 3. ì œëª© ì¶”ì¶œ (ì´ë¯¸ì§€ altê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ)
                    if not title:
                        name_tag = item.select_one('.name, .title, .prod_name, p')
                        if name_tag:
                            title = name_tag.get_text(strip=True)
                    
                    if not title:
                        # í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ì°¾ê¸°
                        text = item.get_text(strip=True)
                        if len(text) > 2:
                            title = text.split('ì›')[0].strip()[-20:] # ì¶”ì¸¡
                    
                    if not title or len(title) < 2:
                        continue

                    # 4. ê°€ê²© ì¶”ì¶œ
                    price = 0
                    price_tag = item.select_one('.price, .cost, .val')
                    price_text = price_tag.get_text() if price_tag else item.get_text()
                    
                    numbers = re.findall(r'\d+', price_text.replace(',', ''))
                    if numbers:
                        # ê°€ì¥ í° ìˆ«ìë¥¼ ê°€ê²©ìœ¼ë¡œ ê°„ì£¼
                        price = max([int(n) for n in numbers if len(n) < 7])
                    
                    # 5. ë§í¬ ì¶”ì¶œ
                    link_tag = item.find('a')
                    source_url = category_url
                    if link_tag and link_tag.get('href') and 'javascript' not in link_tag.get('href'):
                        href = link_tag.get('href')
                        if href.startswith('http'):
                            source_url = href
                        else:
                            source_url = self.base_url + href
                            
                    # ê²°ê³¼ ì¶”ê°€
                    product = {
                        'brand_id': self.brand_id,
                        'title': title,
                        'normalized_title': self.normalize_title(title),
                        'price': price,
                        'category': category_name,
                        'launch_date': datetime.now().date().isoformat(),
                        'image_url': image_url,
                        'source_url': source_url,
                        'is_active': True
                    }
                    
                    products.append(product)
                    print(f"    âœ“ {title} ({price}ì›)")
                    
                except Exception as e:
                    continue
            
        except Exception as e:
            print(f"  âŒ {category_name} ì˜¤ë¥˜: {e}")
        
        return products
    
    def crawl(self):
        print("ğŸª CU ì‹ ì œí’ˆ í¬ë¡¤ë§ ì‹œì‘...")
        driver = None
        all_products = []
        
        try:
            driver = self.setup_driver()
            
            for url, name in self.category_urls:
                products = self.crawl_category(driver, url, name)
                all_products.extend(products)
                time.sleep(2)
            
            return all_products
            
        except Exception as e:
            print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {e}")
            return []
            
        finally:
            if driver:
                driver.quit()
    
    def normalize_title(self, title):
        normalized = re.sub(r'\s+', ' ', title)
        normalized = re.sub(r'[^\w\sê°€-í£]', '', normalized)
        return normalized.strip().upper()
    
    def save_to_db(self, products):
        if not products:
            return 0
        
        print(f"\nğŸ’¾ DB ì €ì¥ ì‹œì‘... ({len(products)}ê°œ)")
        try:
            thirty_days_ago = (datetime.now() - timedelta(days=30)).date().isoformat()
            
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì•ˆí•˜ê³  ê·¸ëƒ¥ ì €ì¥ ì‹œë„ (ì¤‘ë³µì€ DBì—ì„œ ì²˜ë¦¬í•˜ê±°ë‚˜ ë¬´ì‹œ)
            # ê°„ë‹¨í•˜ê²Œ í•˜ê¸° ìœ„í•´ ìµœê·¼ ë°ì´í„°ë§Œ í™•ì¸
            
            self.supabase.table('new_products').upsert(products, on_conflict='normalized_title, launch_date').execute()
            print(f"âœ… ì €ì¥ ì™„ë£Œ!")
            return len(products)
                
        except Exception as e:
            # upsert ì‹¤íŒ¨ ì‹œ ê°œë³„ insert ì‹œë„
            success_count = 0
            for p in products:
                try:
                    self.supabase.table('new_products').insert(p).execute()
                    success_count += 1
                except:
                    pass
            print(f"âœ… {success_count}ê°œ ì €ì¥ ì™„ë£Œ (ê°œë³„)")
            return success_count

def main():
    try:
        crawler = CUCrawler()
        products = crawler.crawl()
        crawler.save_to_db(products)
    except Exception as e:
        print(e)
        exit(1)

if __name__ == "__main__":
    main()
