import os
import time
import re
import requests
from bs4 import BeautifulSoup
from supabase import create_client

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

MAX_SEARCH_PAGES = 100
CHUNK_SIZE = 50

# âœ… CU ëª¨ë“  ì¹´í…Œê³ ë¦¬
CATEGORIES = [
    {"id": "40", "name": "ì•„ì´ìŠ¤í¬ë¦¼"},
    {"id": "30", "name": "ê³¼ìë¥˜"},
    {"id": "10", "name": "ê°„í¸ì‹ì‚¬"},
    {"id": "11", "name": "ì¦‰ì„ì¡°ë¦¬"},
    {"id": "20", "name": "ì‹í’ˆ"},
    {"id": "60", "name": "ìŒë£Œ"},
    {"id": "50", "name": "ìƒí™œìš©í’ˆ"},
]
def parse_product(item, category_name):
    """ìƒí’ˆ íŒŒì‹±"""
    try:
        name_tag = item.select_one(".name p")
        title = (name_tag.get_text(strip=True) if name_tag else "").strip()
        
        price_tag = item.select_one(".price strong")
        price_text = (price_tag.get_text(strip=True) if price_tag else "0").replace(",", "").replace("ì›", "")
        price = int(price_text) if price_text.isdigit() else 0

        img_tag = item.select_one("img")
        image_url = ""
        if img_tag:
            image_url = img_tag.get("src") or img_tag.get("data-src") or ""
            if image_url.startswith("//"):
                image_url = "https:" + image_url
            elif image_url.startswith("/"):
                image_url = "https://cu.bgfretail.com" + image_url

        badge_tag = item.select_one(".badge")
        promotion_type = badge_tag.get_text(strip=True) if badge_tag else None

        # âœ… gdIdx ì¶”ì¶œ
        gdIdx = None
        onclick_div = item.select_one("div[onclick*='view']")
        if onclick_div:
            onclick = onclick_div.get("onclick", "")
            m = re.search(r"view\s*\(\s*(\d+)\s*\)", onclick)
            if m:
                gdIdx = int(m.group(1))
        
        product_url = f"https://cu.bgfretail.com/product/view.do?gdIdx={gdIdx}&category=product" if gdIdx else "https://cu.bgfretail.com/product/view.do?category=product"
        
        if not title:
            return None

        return {
            "title": title,
            "price": price,
            "image_url": image_url,
            "category": category_name,
            "promotion_type": promotion_type,
            "source_url": product_url,
            "is_active": True,
            "brand_id": 1,
            "external_id": gdIdx
        }
    except Exception as e:
        print(f"íŒŒì‹± ì—ëŸ¬: {e}")
        return None

def fetch_new_products(supabase, category_id, category_name, max_gdIdx):
    """ì‹ ìƒí’ˆë§Œ í¬ë¡¤ë§"""
    new_products = []
    print(f"\nğŸ”„ [{category_name}] max_gdIdx={max_gdIdx}ë³´ë‹¤ í° ìƒí’ˆ ì°¾ê¸°...")
    
    for page in range(1, MAX_SEARCH_PAGES + 1):
        url = "https://cu.bgfretail.com/product/productAjax.do"
        payload = {
            "pageIndex": page, 
            "searchMainCategory": category_id,
            "listType": 0
        }
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        }

        try:
            r = requests.post(url, data=payload, headers=headers, timeout=8)
            r.encoding = "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            items = soup.select("li.prod_list")

            if not items:
                print(f"  ğŸ›‘ í˜ì´ì§€ {page}: ë! (ì´ {len(new_products)}ê°œ)")
                break
            
            count_in_page = 0
            for item in items:
                p = parse_product(item, category_name)
                if p and p['external_id'] is not None:
                    if p['external_id'] > max_gdIdx:
                        new_products.append(p)
                        count_in_page += 1
            
            if count_in_page > 0:
                print(f"  âœ… í˜ì´ì§€ {page}: ì‹ ìƒí’ˆ {count_in_page}ê°œ (ëˆ„ì  {len(new_products)})")
            else:
                print(f"  PASS í˜ì´ì§€ {page}")
            
            time.sleep(0.1)
            
        except Exception as e:
            print(f"  âŒ í˜ì´ì§€ {page}: {e}")
            break
    
    return new_products

def remove_duplicates(products):
    """external_id ê¸°ì¤€ ì¤‘ë³µ ì œê±°"""
    unique = {}
    for p in products:
        if p['external_id'] not in unique:
            unique[p['external_id']] = p
    
    result = list(unique.values())
    if len(products) != len(result):
        print(f"  ì¤‘ë³µ ì œê±°: {len(products)} â†’ {len(result)}ê°œ")
    return result

def chunk(lst, size):
    """ì²­í¬ ë‚˜ëˆ„ê¸°"""
    for i in range(0, len(lst), size):
        yield lst[i:i+size]

def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")

    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    total_saved = 0
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
    for cat in CATEGORIES:
        cat_id = cat["id"]
        cat_name = cat["name"]
        
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ ì¹´í…Œê³ ë¦¬: {cat_name} (ID: {cat_id})")
        print(f"{'='*60}")
        
        # 1. ìµœëŒ€ external_id ì¡°íšŒ (NULL ì œì™¸)
        try:
            last_item = supabase.table("new_products") \
                .select("external_id") \
                .eq("brand_id", 1) \
                .eq("category", cat_name) \
                .not_.is_("external_id", None) \
                .order("external_id", desc=True) \
                .limit(1) \
                .execute()
            
            max_gdIdx = last_item.data[0]['external_id'] if last_item.data else 0
            print(f"ğŸ“Š í˜„ì¬ DB ë§ˆì§€ë§‰ ìƒí’ˆ ë²ˆí˜¸: {max_gdIdx}")
            
        except Exception as e:
            print(f"DB ì¡°íšŒ ì—ëŸ¬: {e}")
            max_gdIdx = 0

        # 2. ì‹ ìƒí’ˆ í¬ë¡¤ë§
        raw_products = fetch_new_products(supabase, cat_id, cat_name, max_gdIdx)

        if not raw_products:
            print(f"âœ¨ {cat_name}ì— ìƒˆë¡œìš´ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # 3. ì¤‘ë³µ ì œê±°
        unique_products = remove_duplicates(raw_products)

        # 4. ì €ì¥
        if unique_products:
            print(f"\nğŸ’¾ {len(unique_products)}ê°œ ì €ì¥ ì¤‘...")
            
            saved_count = 0
            for chunk_list in chunk(unique_products, CHUNK_SIZE):
                try:
                    supabase.table("new_products").insert(chunk_list).execute()
                    saved_count += len(chunk_list)
                    print(f"  {saved_count}/{len(unique_products)} ì €ì¥ ì™„ë£Œ")
                except Exception as e:
                    print(f"  ì €ì¥ ì‹¤íŒ¨: {e}")
                    break
            
            print(f"ğŸ‰ {cat_name} ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
            if unique_products:
                print(f"   - ìµœì‹  1ìœ„: {unique_products[-1]['title']}")
                print(f"   - ìµœì‹  2ìœ„: {unique_products[-2]['title'] if len(unique_products)>1 else 'ì—†ìŒ'}")
            
            total_saved += saved_count
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {total_saved}ê°œ ì €ì¥ë¨")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()

