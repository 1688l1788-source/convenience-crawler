import os
import re
import time
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

try:
    from supabase import create_client, Client
except ImportError:
    from supabase import create_client

class CUCrawler:
    def __init__(self):
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_SERVICE_KEY')
        
        if not supabase_url or not supabase_key:
            raise Exception("Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            self.supabase = create_client(supabase_url, supabase_key)
        except TypeError:
            self.supabase = create_client(
                supabase_url=supabase_url,
                supabase_key=supabase_key
            )
        
        self.brand_id = 1
        self.base_url = "https://cu.bgfretail.com"
        
        # ì¹´í…Œê³ ë¦¬ URL ë§¤í•‘
        self.category_urls = [
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=1", "ê°„í¸ì‹ì‚¬"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=2", "ê³¼ìë¥˜"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=3", "ì•„ì´ìŠ¤í¬ë¦¼"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=4", "ì‹í’ˆ"),
            ("https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=5", "ìŒë£Œ"),
        ]
    
    def setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    
    def crawl_category(self, driver, category_url, category_name):
        print(f"\nğŸ“‚ {category_name} í¬ë¡¤ë§ ì¤‘...")
        products = []
        
        try:
            driver.get(category_url)
            time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¦ê°€
            
            # í˜ì´ì§€ HTML ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print(f"  ğŸ” í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´: {len(driver.page_source)}")
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
            selectors = [
                'ul.prodListWrap > li',
                '.prod_list > li',
                'div.prod_list li',
                'li.prod_item',
                '.prodList li',
                'div[class*="prod"] li'
            ]
            
            product_items = []
            for selector in selectors:
                product_items = soup.select(selector)
                if product_items:
                    print(f"  âœ… ì„ íƒì '{selector}' ë¡œ {len(product_items)}ê°œ ë°œê²¬")
                    break
            
            if not product_items:
                print(f"  âŒ ì œí’ˆ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                # HTML ì¼ë¶€ ì¶œë ¥
                print(f"  ğŸ“„ HTML ìƒ˜í”Œ: {str(soup)[:500]}")
                return []
            
            # ìƒìœ„ 10ê°œë§Œ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©)
            for idx, item in enumerate(product_items[:10]):
                try:
                    # ë§í¬ ì°¾ê¸°
                    link = item.find('a')
                    if not link or not link.get('href'):
                        continue
                    
                    href = link.get('href')
                    
                    # ì œí’ˆëª… ì°¾ê¸° (ë‹¤ì–‘í•œ ë°©ë²•)
                    title = None
                    if link.get('title'):
                        title = link.get('title').strip()
                    elif item.find(string=True):
                        title = item.get_text().strip()[:50]
                    
                    if not title or len(title) < 3:
                        continue
                    
                    # ê°€ê²© ì°¾ê¸° (ìˆ«ìë§Œ)
                    price = None
                    price_text = item.get_text()
                    numbers = re.findall(r'\d+', price_text.replace(',', ''))
                    if numbers:
                        # ê°€ì¥ í° ìˆ«ìë¥¼ ê°€ê²©ìœ¼ë¡œ (ë³´í†µ ê°€ê²©ì´ ê°€ì¥ í¼)
                        price = max(int(n) for n in numbers if len(n) <= 6)
                    
                    # ì´ë¯¸ì§€ URL
                    image_url = None
                    img = item.find('img')
                    if img:
                        image_url = img.get('src') or img.get('data-src')
                        if image_url and not image_url.startswith('http'):
                            image_url = self.base_url + image_url
                    
                    # ìƒì„¸ URL
                    if href.startswith('http'):
                        source_url = href
                    elif href.startswith('/'):
                        source_url = self.base_url + href
                    else:
                        source_url = self.base_url + '/' + href
                    
                    product = {
                        'brand_id': self.brand_id,
                        'title': title,
                        'normalized_title': self.normalize_title(title),
                        'price': price,
                        'category': category_name,
                        'launch_date': datetime.now().date().isoformat(),
                        'image_url': image_url,
                        'source_url': source_url,
                        'is_active': True
                    }
                    
                    products.append(product)
                    print(f"    âœ“ {idx+1}. {title[:30]} ({price}ì›)")
                    
                except Exception as e:
                    print(f"    âœ— {idx+1}ë²ˆ íŒŒì‹± ì˜¤ë¥˜: {e}")
            
        except Exception as e:
            print(f"  âŒ {category_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        return products
    
    def crawl(self):
        print("ğŸª CU ì‹ ì œí’ˆ í¬ë¡¤ë§ ì‹œì‘...")
        driver = None
        all_products = []
        
        try:
            driver = self.setup_driver()
            
            for url, name in self.category_urls:
                products = self.crawl_category(driver, url, name)
                all_products.extend(products)
                time.sleep(3)
            
            print(f"\nâœ… ì´ {len(all_products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
            return all_products
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
            
        finally:
            if driver:
                driver.quit()
    
    def normalize_title(self, title):
        normalized = re.sub(r'\s+', ' ', title)
        normalized = re.sub(r'[^\w\sê°€-í£]', '', normalized)
        return normalized.strip().upper()
    
    def save_to_db(self, products):
        if not products:
            print("ğŸ’¤ ì €ì¥í•  ì œí’ˆ ì—†ìŒ")
            return 0
        
        print(f"\nğŸ’¾ DB ì €ì¥ ì‹œì‘... ({len(products)}ê°œ)")
        
        try:
            thirty_days_ago = (datetime.now() - timedelta(days=30)).date().isoformat()
            existing = self.supabase.table('new_products')\
                .select('normalized_title, launch_date')\
                .eq('brand_id', self.brand_id)\
                .gte('launch_date', thirty_days_ago)\
                .execute()
            
            existing_keys = {
                f"{p['normalized_title']}_{p['launch_date']}" 
                for p in existing.data
            }
            
            new_products = [
                p for p in products 
                if f"{p['normalized_title']}_{p['launch_date']}" not in existing_keys
            ]
            
            if new_products:
                self.supabase.table('new_products').insert(new_products).execute()
                print(f"âœ… {len(new_products)}ê°œ ì €ì¥ ì™„ë£Œ!")
                return len(new_products)
            else:
                print("â„¹ï¸ ëª¨ë‘ ê¸°ì¡´ ì œí’ˆ")
                return 0
                
        except Exception as e:
            print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return 0

def main():
    print("="*60)
    print("ğŸª CU ì‹ ì œí’ˆ í¬ë¡¤ëŸ¬ (Selenium)")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    try:
        crawler = CUCrawler()
        products = crawler.crawl()
        new_count = crawler.save_to_db(products)
        
        print("\n" + "="*60)
        print(f"âœ¨ ì™„ë£Œ!")
        print(f"ğŸ“Š ìˆ˜ì§‘: {len(products)}ê°œ | ì‹ ê·œ: {new_count}ê°œ")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
