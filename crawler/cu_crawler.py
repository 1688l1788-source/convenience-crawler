import os
import time
import re
import json
import requests
from bs4 import BeautifulSoup
from supabase import create_client

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

# ==========================================
# ğŸ§  í†µí•© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸° (ìˆœì„œ ë° ì˜ˆì™¸ì²˜ë¦¬ ê°•í™”)
# ==========================================
def get_standard_category(title, raw_category=None):
    """
    ìƒí’ˆëª…ì„ ë¶„ì„í•˜ì—¬ ì•± í‘œì¤€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    ê²€ì‚¬ ìˆœì„œ: ìƒí™œìš©í’ˆ > ì‹ì‚¬ > ìŒë£Œ > ê³¼ì > ì•„ì´ìŠ¤ (ì˜¤ë¶„ë¥˜ ë°©ì§€)
    """
    
    # 1. ìƒí™œìš©í’ˆ (ê°€ì¥ ëª…í™•í•˜ë¯€ë¡œ ìµœìš°ì„ )
    if any(k in title for k in [
        'ì¹˜ì•½', 'ì¹«ì†”', 'ê°€ê¸€', 'ê°€ê·¸ë¦°', 'í˜ë¦¬ì˜¤', 'ë©”ë””ì•ˆ', '2080', 'ë¦¬ì¹˜', 'ë´íƒˆ', 'ë§ˆìš°ìŠ¤', 'ì‰ì´ë¹™', 'ë©´ë„ê¸°',
        'ë¬¼í‹°ìŠˆ', 'í‹°ìŠˆ', 'ë§ˆìŠ¤í¬', 'ìƒë¦¬ëŒ€', 'ì¤‘í˜•', 'ëŒ€í˜•', 'ì†Œí˜•', 'ì˜¤ë²„ë‚˜ì´íŠ¸', 'ì…ëŠ”ì˜¤ë²„', 'íŒ¨ë“œ', 'ë¼ì´ë„ˆ', 'íƒí°', 'íŒ¬í‹°',
        'ë¼ì—˜', 'ì˜í”¼', 'í™”ì´íŠ¸', 'ì¢‹ì€ëŠë‚Œ', 'ì‹œí¬ë¦¿ë°ì´', 'ì• ë‹ˆë°ì´', 'ë””ì–´ìŠ¤í‚¨', 'ìˆœìˆ˜í•œë©´',
        'ìƒ´í‘¸', 'ë¦°ìŠ¤', 'íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸', 'í—¤ì–´', 'ì„¸ëŸ¼', 'ë¹„ëˆ„', 'ì—˜ë¼ìŠ¤í‹´', 'ì¼€ë¼ì‹œìŠ¤', 'ì˜¤ê°€ë‹ˆìŠ¤íŠ¸', 'ì˜¨ë”ë°”ë””', 'ë°”ë””ì›Œì‹œ',
        'ë¡œì…˜', 'í•¸ë“œí¬ë¦¼', 'ìˆ˜ë”©ì ¤', 'í´ë Œì§•', 'ì›Œí„°ë§ˆì´ë“œ', 'ì—ì„¼ì…œ', 'ì¡´ìŠ¨ì¦ˆ', 'ì•„ë¹„ë…¸', 'ë‹ˆë² ì•„', 'ë©”ë””í', 'ë¦½ì¼€ì–´', 'ì˜¤ì¼',
        'ì„¸ì œ', 'ë½ìŠ¤', 'ìŠˆê°€ë²„ë¸”', 'ë¬´ê· ë¬´ë•Œ', 'íí', 'í”¼ì§€', 'ê±´ì „ì§€', 'ìŠ¤íƒ€í‚¹', 'ë°´ë“œ', 'ì¼íšŒìš©', 'ì œê±°', 'í´ë¦°í•', 'ìš°ì‚°', 'ì–‘ë§', 'ë°”ë””'
    ]):
        return "ìƒí™œìš©í’ˆ"

    # 2. ì‹ì‚¬/ë¼ë©´ (í–„ë²„ê±°, ìƒŒë“œìœ„ì¹˜, ë°¥ë°” ë“± 'ë°”'ë‚˜ 'ì½˜'ì´ ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ì‹í’ˆ ë¨¼ì € ì²˜ë¦¬)
    if any(k in title for k in [
        'ë„ì‹œë½', 'ê¹€ë°¥', 'ì£¼ë¨¹ë°¥', 'ë°¥ë°”', 'ìƒŒë“œìœ„ì¹˜', 'ìƒŒë“œ', 'í–„ë²„ê±°', 'ë²„ê±°', 'ìƒëŸ¬ë“œ', 'ì•¼ë¼ì†Œë°”',
        'ë¼ë©´', 'ë©´', 'ìš°ë™', 'êµ­ë°¥', 'ì£½', 'íƒ•', 'ì°Œê°œ', 'êµ­', 'ì»µë°˜', 'í–‡ë°˜', 
        'í•«ë°”', 'ì†Œì‹œì§€', 'í›„ë‘í¬', 'ë§Œë‘', 'ë‹­ê°€ìŠ´ì‚´', 'ì¹˜í‚¨', 'ìœ¡ê°œì¥', 'í­íƒ„ì½˜', 'ë² ì´ì»¨', 'ìŠ¤í…Œì´í¬',
        'ê·¸ë˜ë†€ë¼', 'í†µê³¡ë¬¼ë°¥', 'í¬ë©', 'íŠ€ê¹€', 'ë¸Œë¦¬ë˜', 'íŒŒìŠ¤íƒ€', '3XL', 'í‚¬ë°”ì‚¬', 'ì˜¤ì§•ì–´'
    ]):
        return "ì‹ì‚¬/ë¼ë©´"

    # 3. ìŒë£Œ ('ì»¤í”¼', 'ë¼ë–¼'ê°€ ì•„ì´ìŠ¤í¬ë¦¼ìœ¼ë¡œ ì˜¤ë¶„ë¥˜ë˜ëŠ” ê²ƒ ë°©ì§€)
    if any(k in title for k in [
        'ìš°ìœ ', 'ì»¤í”¼', 'ë¼ë–¼', 'ì•„ë©”ë¦¬ì¹´ë…¸', 'ì½œë¼', 'ì‚¬ì´ë‹¤', 'ì—ì´ë“œ', 'ì£¼ìŠ¤', 'ë³´ë¦¬ì°¨', 'ì˜¥ìˆ˜ìˆ˜ìˆ˜ì—¼ì°¨', 
        'ë¹„íƒ€', 'ë°•ì¹´ìŠ¤', 'ìŒí™”', 'ë‘ìœ ', 'ìš”êµ¬ë¥´íŠ¸', 'ìš”ê±°íŠ¸', 'ë¬¼', 'ì›Œí„°', 'í”„ë¡œí‹´', 'ì½¤ë¶€ì°¨', 'ë“œë§í¬', 'ì´ì˜¨', 
        'í‹°', 'TEA', 'ë°”ë¦¬ìŠ¤íƒ€', 'ì½˜íŠ¸ë¼', 'ì¹´í˜', 'ë§ˆì´ë…¸ë©€', 'ì„œìš¸FB'
    ]):
        return "ìŒë£Œ"

    # 4. ê³¼ì/ê°„ì‹ ('ê¼¬ê¹”ì½˜', 'ì—ë„ˆì§€ë°”' ë“± ì˜¤ë¶„ë¥˜ ë°©ì§€)
    if any(k in title for k in [
        'ê¼¬ê¹”ì½˜', 'ì¹˜í† ìŠ¤', 'ì½˜ì´ˆ', 'ì½”ì½”ë³¼', 'ì½˜í‘¸ë¼ì´íŠ¸', 'ë‹¨ë°±ì§ˆë°”', 'ì—ë„ˆì§€ë°”', 'ì´ˆì½”ë°”', 'í¬ëŸ°í‚¤',
        'ìŠ¤ë‚µ', 'ì ¤ë¦¬', 'ì‚¬íƒ•', 'ê»Œ', 'ì´ˆì½”', 'ì¿ í‚¤', 'ì¹©', 'ë¹µ', 'ì¼€ìµ', 'ì•½ê³¼', 'ì–‘ê°±', 'í”„ë ˆì²¼', 'íŒì½˜', 
        'ì•„ëª¬ë“œ', 'ìœ¡í¬', 'ì–´ë¬µ', 'ë§›ë°¤', 'ë§ì°¨ë¹µ', 'í—ˆì‰¬', 'ê·¸ë¦­ìš”ê±°íŠ¸', 'ì˜¤íŒœ', 'í‘¸ë”©', 'ë””ì €íŠ¸', 'í‚·ìº£', 'ë„ë„›'
    ]):
        return "ê³¼ì/ê°„ì‹"

    # 5. ì•„ì´ìŠ¤ (ìœ„ì—ì„œ ì•ˆ ê±¸ëŸ¬ì§„ ê²ƒë“¤ ì¤‘ ì•„ì´ìŠ¤í¬ë¦¼ í‚¤ì›Œë“œ)
    if any(k in title for k in [
        'ì•„ì´ìŠ¤', 'íŒŒì¸íŠ¸', 'í•˜ê²ë‹¤ì¦ˆ', 'ë‚˜ëšœë£¨', 'ì„¤ë ˆì„', 'í´ë¼í¬', 'ìŠ¤í¬ë¥˜', 'ë¼ì§€ë°”', 'ë¹™ìˆ˜', 'ìƒ¤ë² íŠ¸', 
        'ë¯¸ë‹ˆì»µ', 'ë¹„ë¹„ë¹…', 'ë©”ë¡œë‚˜', 'ëˆ„ê°€ë°”', 'ìŒìŒë°”', 'ë°”ë°¤ë°”', 'ì˜¥ë™ì', 'ì™€ì¼ë“œë°”ë””', 'ë¶•ì–´ì‹¸ë§Œì½”', 
        'ë”ìœ„ì‚¬ëƒ¥', 'ë¹µë¹ ë ˆ', 'êµ¬ìŠ¬', 'ì†Œí”„íŠ¸ì½˜', 'íƒ±í¬ë³´ì´', 'ë¹ ì‚ì½”', 'ìš”ë§˜ë•Œ', 'ì¿ ì•¤í¬', 'ìˆ˜ë°•ë°”', 'ì£ ìŠ¤ë°”', 
        'ë²¤ì•¤', 'ë¼ë¼ìŠ¤ìœ—', 'ì œë¡œìœ—', 'ë¡œìš°ìœ—', 'ì„œì£¼', 'ë™ê·¸ë¦°', 'ì‚¼ìš°'
    ]):
        return "ì•„ì´ìŠ¤"
    
    # 5-1. ì•„ì´ìŠ¤ ë³´ì¡° (ë‹¨ì–´ 'ì½˜', 'ë°”'ëŠ” ìœ„í—˜í•˜ë¯€ë¡œ ì¡°ê±´ë¶€ ê²€ì‚¬)
    # ìœ„ ì‹ì‚¬/ê³¼ì ë‹¨ê³„ì—ì„œ 'ê¼¬ê¹”ì½˜', 'í•«ë°”' ë“±ì´ ì´ë¯¸ ê±¸ëŸ¬ì¡Œìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¹„êµì  ì•ˆì „
    if title.endswith('ë°”') or title.endswith('ì½˜') or 'íŒŒë¥´í˜' in title:
        return "ì•„ì´ìŠ¤"

    # 6. CU ì›ë³¸ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ìµœí›„ì˜ ë³´ë£¨)
    if raw_category:
        if raw_category in ["ê°„í¸ì‹ì‚¬", "ì¦‰ì„ì¡°ë¦¬", "ì‹í’ˆ"]: return "ì‹ì‚¬/ë¼ë©´"
        if raw_category == "ê³¼ìë¥˜": return "ê³¼ì/ê°„ì‹"
        if raw_category == "ì•„ì´ìŠ¤í¬ë¦¼": return "ì•„ì´ìŠ¤"
        if raw_category == "ìƒí™œìš©í’ˆ": return "ìƒí™œìš©í’ˆ"
        if raw_category == "ìŒë£Œ": return "ìŒë£Œ"

    return "ê¸°íƒ€"

# ==========================================
# ğŸª 1. CU í¬ë¡¤ë§ (GD_xx -> ìˆ«ì ì½”ë“œ ì‚¬ìš©)
# ==========================================
def parse_cu_product(item, raw_cat_name):
    try:
        name_tag = item.select_one(".name p")
        if not name_tag: return None
        title = name_tag.get_text(strip=True)
        
        price_tag = item.select_one(".price strong")
        price = int(price_tag.get_text(strip=True).replace(",", "")) if price_tag else 0
        
        img_tag = item.select_one("img")
        img_src = ""
        if img_tag:
            img_src = img_tag.get("src") or ""
            if img_src and not img_src.startswith("http"):
                img_src = "https:" + img_src
        
        badge = item.select_one(".badge")
        promo = badge.get_text(strip=True) if badge else "í–‰ì‚¬"
        if promo not in ["1+1", "2+1"]: return None # í–‰ì‚¬ ìƒí’ˆë§Œ

        # ID ì¶”ì¶œ
        gdIdx = None
        onclick_div = item.select_one("div[onclick*='view']")
        if onclick_div:
            m = re.search(r"view\s*\(\s*(\d+)\s*\)", onclick_div.get("onclick", ""))
            if m: gdIdx = int(m.group(1))
        
        if not gdIdx: return None

        # ì •ë°€ ë¶„ë¥˜ ì ìš©
        std_category = get_standard_category(title, raw_cat_name)
        
        return {
            "title": title,
            "price": price,
            "image_url": img_src,
            "category": std_category,
            "original_category": raw_cat_name,
            "promotion_type": promo,
            "brand_id": 1,
            "source_url": f"https://cu.bgfretail.com/product/view.do?category=product&gdIdx={gdIdx}",
            "is_active": True,
            "external_id": gdIdx
        }
    except: return None

def crawl_cu(supabase):
    print("\nğŸš€ CU í¬ë¡¤ë§ ì‹œì‘...")
    
    # ì¹´í…Œê³ ë¦¬ ì½”ë“œ (ìˆ«ì)
    cu_categories = {
        "10": "ê°„í¸ì‹ì‚¬", "20": "ì¦‰ì„ì¡°ë¦¬", "30": "ê³¼ìë¥˜",
        "40": "ì•„ì´ìŠ¤í¬ë¦¼", "50": "ì‹í’ˆ", "60": "ìŒë£Œ", "70": "ìƒí™œìš©í’ˆ"
    }
    
    all_items = []
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://cu.bgfretail.com"
    }
    
    for code, name in cu_categories.items():
        print(f"ğŸ” CU ì¡°íšŒ: {name}")
        # DBì—ì„œ ìµœì‹  ID ì¡°íšŒ
        try:
            res = supabase.table("new_products").select("external_id").eq("brand_id", 1).eq("original_category", name).order("external_id", desc=True).limit(1).execute()
            max_id = res.data[0]['external_id'] if res.data else 0
        except: max_id = 0

        for page in range(1, 20):
            try:
                r = requests.post("https://cu.bgfretail.com/product/productAjax.do", 
                                data={"pageIndex": page, "searchMainCategory": code, "listType": 1}, 
                                headers=headers, timeout=10)
                r.encoding = 'utf-8'
                soup = BeautifulSoup(r.text, "html.parser")
                items = soup.select("li.prod_list")
                
                if not items: break
                
                page_count = 0
                for item in items:
                    p = parse_cu_product(item, name)
                    if p:
                        # ì¦ë¶„: DBë³´ë‹¤ ìƒˆë¡œìš´ ìƒí’ˆë§Œ (ë‹¨, DBê°€ ë¹„ì—ˆìœ¼ë©´ ëª¨ë‘)
                        if max_id == 0 or p['external_id'] > max_id:
                            all_items.append(p)
                            page_count += 1
                
                if page_count == 0 and max_id > 0: break # ë” ì´ìƒ ì‹ ê·œ ì—†ìŒ
                time.sleep(0.1)
            except: break

    if all_items:
        print(f"âœ… CU {len(all_items)}ê°œ ì‹ ê·œ ì €ì¥ ì¤‘...")
        # ì¤‘ë³µ ì œê±°
        unique = {p['external_id']: p for p in all_items}.values()
        for i in range(0, len(list(unique)), 100):
            supabase.table("new_products").upsert(list(unique)[i:i+100], on_conflict="external_id,brand_id").execute()
        print("ğŸ‰ CU ì™„ë£Œ!")
    else:
        print("âœ¨ CU ìµœì‹  ìƒíƒœ.")

# ==========================================
# ğŸª 2. GS25 í¬ë¡¤ë§
# ==========================================
def crawl_gs25(supabase):
    print("\nğŸš€ GS25 í¬ë¡¤ë§ ì‹œì‘...")
    
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://gs25.gsretail.com/gscvs/ko/products/event-goods",
        "X-Requested-With": "XMLHttpRequest"
    })
    
    token = None
    try:
        r = session.get("https://gs25.gsretail.com/gscvs/ko/products/event-goods")
        token = BeautifulSoup(r.text, "html.parser").find("input", {"name": "CSRFToken"})['value']
    except:
        # ë°±ì—… ë°©ì‹
        try:
            m = re.search(r"CSRFToken\s*[:=]\s*['\"]([^'\"]+)['\"]", r.text)
            if m: token = m.group(1)
        except: pass

    if not token:
        print("âŒ GS25 í† í° ì‹¤íŒ¨")
        return

    all_items = []
    
    for p_type in ["ONE_TO_ONE", "TWO_TO_ONE", "GIFT"]:
        print(f"ğŸ” GS25 ì¡°íšŒ: {p_type}")
        for page in range(1, 20):
            try:
                r = session.post("https://gs25.gsretail.com/gscvs/ko/products/event-goods-search",
                               data={"CSRFToken": token, "pageNum": str(page), "pageSize": "50", "parameterList": p_type},
                               timeout=10)
                try: data = r.json()
                except: data = json.loads(r.text)
                
                results = data.get("results", [])
                if not results: break
                
                for item in results:
                    title = item.get("goodsNm", "").strip()
                    price = int(item.get("price", 0))
                    std_cat = get_standard_category(title, None)
                    
                    id_match = re.search(r'(\d+)', item.get("attFileId", ""))
                    ext_id = int(id_match.group(1)[-18:]) if id_match else int(time.time()*1000)
                    
                    promo_map = {"ONE_TO_ONE": "1+1", "TWO_TO_ONE": "2+1", "GIFT": "ë¤ì¦ì •"}
                    
                    all_items.append({
                        "title": title,
                        "price": price,
                        "image_url": item.get("attFileNm", ""),
                        "category": std_cat,
                        "original_category": None,
                        "promotion_type": promo_map.get(p_type, "í–‰ì‚¬"),
                        "brand_id": 2,
                        "source_url": "http://gs25.gsretail.com/gscvs/ko/products/event-goods",
                        "is_active": True,
                        "external_id": ext_id
                    })
                time.sleep(0.2)
            except: break

    if all_items:
        print(f"âœ… GS25 {len(all_items)}ê°œ ì €ì¥ ì¤‘ (ì „ì²´ ê°±ì‹ )...")
        supabase.table("new_products").delete().eq("brand_id", 2).execute()
        for i in range(0, len(all_items), 100):
            supabase.table("new_products").insert(all_items[i:i+100]).execute()
        print("ğŸ‰ GS25 ì™„ë£Œ!")
    else:
        print("ğŸ˜± GS25 ë°ì´í„° 0ê°œ.")

def main():
    if not SUPABASE_URL or not SUPABASE_KEY: return
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    crawl_cu(supabase)
    crawl_gs25(supabase)
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    main()
