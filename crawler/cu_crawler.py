import os
import re
import time
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

try:
    from supabase import create_client, Client
except ImportError:
    from supabase import create_client

class CUCrawler:
    def __init__(self):
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_SERVICE_KEY')
        
        if not supabase_url or not supabase_key:
            raise Exception("Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            self.supabase = create_client(supabase_url, supabase_key)
        except TypeError:
            self.supabase = create_client(
                supabase_url=supabase_url,
                supabase_key=supabase_key
            )
        
        self.brand_id = 1
        self.base_url = "https://cu.bgfretail.com"
        
        # ì¹´í…Œê³ ë¦¬ URL ë§¤í•‘ (depth3 = 1~7)
        self.category_urls = [
            "https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=1",  # ê°„í¸ì‹ì‚¬
            "https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=2",  # ê³¼ìë¥˜
            "https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=3",  # ì•„ì´ìŠ¤í¬ë¦¼
            "https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=4",  # ì‹í’ˆ
            "https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=5",  # ìŒë£Œ
            "https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=6",  # ìƒí™œìš©í’ˆ
            "https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=7",  # ê±´ê°•/ìœ„ìƒìš©í’ˆ
        ]
    
    def setup_driver(self):
        """Selenium í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    
    def extract_product_details(self, driver, product_url):
        """ê°œë³„ ì œí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            driver.get(product_url)
            time.sleep(2)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # ì œí’ˆëª…
            title = None
            title_elem = soup.select_one('.prodTitle, .prod_title, h3, .name')
            if title_elem:
                title = title_elem.text.strip()
            
            # ê°€ê²©
            price = None
            price_elem = soup.select_one('.prodPrice, .price, .cost, .val')
            if price_elem:
                price_text = price_elem.text.strip()
                numbers = re.findall(r'\d+', price_text.replace(',', ''))
                if numbers:
                    price = int(''.join(numbers))
            
            # ì´ë¯¸ì§€
            image_url = None
            img = soup.select_one('.prodImg img, .prod_img img, .detail_img img')
            if img:
                image_url = img.get('src') or img.get('data-src')
                if image_url:
                    if image_url.startswith('//'):
                        image_url = 'https:' + image_url
                    elif not image_url.startswith('http'):
                        image_url = self.base_url + image_url
            
            return {
                'title': title,
                'price': price,
                'image_url': image_url,
                'source_url': product_url
            }
            
        except Exception as e:
            print(f"  âš ï¸ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def crawl_category(self, driver, category_url, category_name):
        """ì¹´í…Œê³ ë¦¬ë³„ ì‹ ì œí’ˆ í¬ë¡¤ë§"""
        print(f"\nğŸ“‚ {category_name} í¬ë¡¤ë§ ì¤‘...")
        products = []
        
        try:
            driver.get(category_url)
            time.sleep(3)
            
            # ì œí’ˆ ëª©ë¡ ëŒ€ê¸°
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "ul.prodListWrap li, .prod_list li"))
                )
            except:
                print(f"  âš ï¸ {category_name} ì œí’ˆ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨")
                return []
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # ì œí’ˆ ì°¾ê¸°
            product_items = soup.select('ul.prodListWrap > li, .prod_list > li')
            print(f"  ğŸ“¦ ë°œê²¬: {len(product_items)}ê°œ")
            
            # ìƒìœ„ 20ê°œë§Œ ì²˜ë¦¬
            for idx, item in enumerate(product_items[:20]):
                try:
                    # ì œí’ˆ ë§í¬ ì¶”ì¶œ
                    link = item.select_one('a')
                    if not link or not link.get('href'):
                        continue
                    
                    href = link.get('href')
                    
                    # ì ˆëŒ€ URL ìƒì„±
                    if href.startswith('http'):
                        product_url = href
                    elif href.startswith('/'):
                        product_url = self.base_url + href
                    else:
                        product_url = self.base_url + '/' + href
                    
                    # gdIdx í™•ì¸ (ì‹¤ì œ ì œí’ˆ ìƒì„¸ í˜ì´ì§€ì¸ì§€ í™•ì¸)
                    if 'gdIdx=' not in product_url:
                        continue
                    
                    # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    details = self.extract_product_details(driver, product_url)
                    
                    if not details or not details['title']:
                        continue
                    
                    # ì œí’ˆ ì •ë³´ êµ¬ì„±
                    product = {
                        'brand_id': self.brand_id,
                        'title': details['title'],
                        'normalized_title': self.normalize_title(details['title']),
                        'price': details['price'],
                        'category': self.categorize(details['title']),
                        'launch_date': datetime.now().date().isoformat(),
                        'image_url': details['image_url'],
                        'source_url': details['source_url'],
                        'is_active': True
                    }
                    
                    products.append(product)
                    print(f"    âœ“ {idx+1}. {product['title'][:40]}")
                    
                    # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"    âœ— {idx+1}ë²ˆ ì œí’ˆ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
        except Exception as e:
            print(f"  âŒ {category_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        return products
    
    def crawl(self):
        """ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        print("ğŸª CU ì‹ ì œí’ˆ í¬ë¡¤ë§ ì‹œì‘...")
        driver = None
        all_products = []
        
        try:
            driver = self.setup_driver()
            
            category_names = [
                "ê°„í¸ì‹ì‚¬",
                "ê³¼ìë¥˜", 
                "ì•„ì´ìŠ¤í¬ë¦¼",
                "ì‹í’ˆ",
                "ìŒë£Œ",
                "ìƒí™œìš©í’ˆ",
                "ê±´ê°•/ìœ„ìƒìš©í’ˆ"
            ]
            
            # ê° ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
            for url, name in zip(self.category_urls, category_names):
                products = self.crawl_category(driver, url, name)
                all_products.extend(products)
                
                # ì¹´í…Œê³ ë¦¬ ê°„ ëŒ€ê¸°
                time.sleep(2)
            
            print(f"\nâœ… ì´ {len(all_products)}ê°œ ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
            return all_products
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
            
        finally:
            if driver:
                driver.quit()
    
    def normalize_title(self, title):
        """ì œëª© ì •ê·œí™”"""
        normalized = re.sub(r'\s+', ' ', title)
        normalized = re.sub(r'[^\w\sê°€-í£]', '', normalized)
        return normalized.strip().upper()
    
    def categorize(self, title):
        """ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        title_lower = title.lower()
        keywords = {
            'ìŒë£Œ': ['ìŒë£Œ', 'ì£¼ìŠ¤', 'ì»¤í”¼', 'ìš°ìœ ', 'ì°¨', 'ì›Œí„°', 'ì‚¬ì´ë‹¤', 'ì½œë¼', 'ë¼ë–¼', 'ì—ì´ë“œ'],
            'ê³¼ì': ['ê³¼ì', 'ì´ˆì½œë¦¿', 'ì‚¬íƒ•', 'ì ¤ë¦¬', 'ì¿ í‚¤', 'ë¹„ìŠ¤í‚·', 'ìŠ¤ë‚µ', 'ì¹©', 'íŒì½˜'],
            'ì¦‰ì„ì‹í’ˆ': ['ë„ì‹œë½', 'ê¹€ë°¥', 'ìƒŒë“œìœ„ì¹˜', 'ì‚¼ê°ê¹€ë°¥', 'í•«ë„ê·¸', 'í–„ë²„ê±°', 'ì»µë°¥', 'ë®ë°¥'],
            'ë¼ë©´': ['ë¼ë©´', 'ì»µë¼ë©´', 'ì§œíŒŒê²Œí‹°', 'ì§œì¥ë©´', 'ìš°ë™'],
            'ì•„ì´ìŠ¤í¬ë¦¼': ['ì•„ì´ìŠ¤í¬ë¦¼', 'ë¹™ê³¼', 'ì•„ì´ìŠ¤ë°”', 'ì½˜', 'ë¹™ìˆ˜']
        }
        
        for category, words in keywords.items():
            if any(word in title_lower for word in words):
                return category
        return 'ê¸°íƒ€'
    
    def save_to_db(self, products):
        """Supabaseì— ì €ì¥"""
        if not products:
            print("ğŸ’¤ ì €ì¥í•  ì œí’ˆ ì—†ìŒ")
            return 0
        
        print(f"\nğŸ’¾ DB ì €ì¥ ì‹œì‘... ({len(products)}ê°œ)")
        
        try:
            # ì¤‘ë³µ ì²´í¬ (ìµœê·¼ 30ì¼)
            thirty_days_ago = (datetime.now() - timedelta(days=30)).date().isoformat()
            existing = self.supabase.table('new_products')\
                .select('normalized_title, launch_date')\
                .eq('brand_id', self.brand_id)\
                .gte('launch_date', thirty_days_ago)\
                .execute()
            
            existing_keys = {
                f"{p['normalized_title']}_{p['launch_date']}" 
                for p in existing.data
            }
            
            new_products = [
                p for p in products 
                if f"{p['normalized_title']}_{p['launch_date']}" not in existing_keys
            ]
            
            if new_products:
                self.supabase.table('new_products').insert(new_products).execute()
                print(f"âœ… {len(new_products)}ê°œ ì €ì¥ ì™„ë£Œ!")
                
                # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
                from collections import Counter
                categories = Counter(p['category'] for p in new_products)
                for cat, count in categories.items():
                    print(f"  - {cat}: {count}ê°œ")
                
                return len(new_products)
            else:
                print("â„¹ï¸ ëª¨ë‘ ê¸°ì¡´ ì œí’ˆ (ì‹ ê·œ ì—†ìŒ)")
                return 0
                
        except Exception as e:
            print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return 0

def main():
    print("="*60)
    print("ğŸª CU ì‹ ì œí’ˆ í¬ë¡¤ëŸ¬ (Selenium)")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    try:
        crawler = CUCrawler()
        products = crawler.crawl()
        new_count = crawler.save_to_db(products)
        
        print("\n" + "="*60)
        print(f"âœ¨ ì™„ë£Œ!")
        print(f"ğŸ“Š ìˆ˜ì§‘: {len(products)}ê°œ | ì‹ ê·œ: {new_count}ê°œ")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
