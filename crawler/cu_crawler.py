import os
import time
import re
import requests
from bs4 import BeautifulSoup
from supabase import create_client

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

MAX_SEARCH_PAGES = 100
CHUNK_SIZE = 50

# âœ… í†µí•© ì¹´í…Œê³ ë¦¬ (ë¸Œëœë“œë³„)
BRANDS = {
    "CU": {
        "id": 1,
        "base_url": "https://cu.bgfretail.com",
        "ajax_url": "https://cu.bgfretail.com/product/productAjax.do",
        "categories": [
            {"id": "40", "name": "ì•„ì´ìŠ¤í¬ë¦¼"},
            {"id": "30", "name": "ê³¼ìë¥˜"},
            {"id": "10", "name": "ê°„í¸ì‹ì‚¬"},
            {"id": "20", "name": "ì‹í’ˆ"},
            {"id": "60", "name": "ìŒë£Œ"},
            {"id": "50", "name": "ìƒí™œìš©í’ˆ"},
        ]
    },
    "GS25": {
        "id": 2,
        "base_url": "https://gs25.gsretail.com",
        "ajax_url": "https://gs25.gsretail.com/goods/goodsListAjax.do", 
        "categories": [
            {"id": "1001", "name": "ì•„ì´ìŠ¤í¬ë¦¼"},  # ì‹¤ì œ IDëŠ” ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸
            {"id": "1002", "name": "ê³¼ìë¥˜"},
            {"id": "1003", "name": "ê°„í¸ì‹ì‚¬"},
            {"id": "1004", "name": "ì‹í’ˆ"},
            {"id": "1005", "name": "ìŒë£Œ"},
            {"id": "1006", "name": "ìƒí™œìš©í’ˆ"},
        ]
    }
}

def parse_product(item, category_name, brand_info):
    """ë¸Œëœë“œë³„ ìƒí’ˆ íŒŒì‹±"""
    try:
        if brand_info["id"] == 1:  # CU
            name_tag = item.select_one(".name p")
            price_tag = item.select_one(".price strong")
            onclick_div = item.select_one("div[onclick*='view']")
            
            title = (name_tag.get_text(strip=True) if name_tag else "").strip()
            price_text = (price_tag.get_text(strip=True) if price_tag else "0").replace(",", "").replace("ì›", "")
            price = int(price_text) if price_text.isdigit() else 0
            
            # CU gdIdx
            gdIdx = None
            if onclick_div:
                onclick = onclick_div.get("onclick", "")
                m = re.search(r"view\\s*\\(\\s*(\\d+)\\s*\\)", onclick)
                if m:
                    gdIdx = int(m.group(1))
            external_id = gdIdx
            
        else:  # GS25
            name_tag = item.select_one(".goods_info .name, .name")
            price_tag = item.select_one(".goods_info .price, .price")
            
            title = (name_tag.get_text(strip=True) if name_tag else "").strip()
            price_text = (price_tag.get_text(strip=True) if price_tag else "0").replace(",", "").replace("ì›", "")
            price = int(price_text) if price_text.isdigit() else 0
            
            # GS25 goodsNo
            external_id = None
            data_goods = item.get("data-goods-no")
            if data_goods:
                external_id = int(data_goods)
            else:
                onclick_div = item.select_one("[onclick]")
                if onclick_div:
                    onclick = onclick_div.get("onclick", "")
                    m = re.search(r"(?:fnDetailView|detail)\s*\(\s*'(\d+)'", onclick)
                    if m:
                        external_id = int(m.group(1))

        # ê³µí†µ ì´ë¯¸ì§€ ì²˜ë¦¬
        img_tag = item.select_one("img")
        image_url = ""
        if img_tag:
            image_url = img_tag.get("src") or img_tag.get("data-src") or ""
            if image_url.startswith("//"):
                image_url = "https:" + image_url
            elif image_url.startswith("/"):
                image_url = brand_info["base_url"] + image_url

        badge_tag = item.select_one(".badge, .ico_event, .event")
        promotion_type = badge_tag.get_text(strip=True) if badge_tag else None
        
        product_url = f"{brand_info['base_url']}/product/view.do?gdIdx={external_id}" if brand_info["id"] == 1 else f"{brand_info['base_url']}/goods/goodsView.do?goodsNo={external_id}"
        
        if not title or external_id is None:
            return None

        return {
            "title": title,
            "price": price,
            "image_url": image_url,
            "category": category_name,
            "promotion_type": promotion_type,
            "source_url": product_url if external_id else brand_info["base_url"],
            "is_active": True,
            "brand_id": brand_info["id"],
            "external_id": external_id
        }
    except Exception as e:
        print(f"íŒŒì‹± ì—ëŸ¬ ({brand_info['id']}): {e}")
        return None

def fetch_new_products(supabase, brand_info, category_id, category_name, max_external_id):
    """ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ë³„ ì‹ ìƒí’ˆ í¬ë¡¤ë§"""
    new_products = []
    brand_name = "CU" if brand_info["id"] == 1 else "GS25"
    
    print(f"\nğŸ”„ [{brand_name} {category_name}] max_id={max_external_id}ë³´ë‹¤ í° ìƒí’ˆ...")
    
    for page in range(1, MAX_SEARCH_PAGES + 1):
        if brand_info["id"] == 1:  # CU
            payload = {
                "pageIndex": page,
                "searchMainCategory": category_id,
                "listType": 0
            }
            headers = {
                "User-Agent": "Mozilla/5.0",
                "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            }
        else:  # GS25
            payload = {
                "nowPage": page,
                "cateNo": category_id,
                "dispCtgryNo": "",
                "searchType": "",
                "searchWord": "",
                "sortType": "01",
                "listLimt": "48"
            }
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
                "Referer": f"{brand_info['base_url']}/goods/goodsList.do",
            }

        try:
            r = requests.post(brand_info["ajax_url"], data=payload, headers=headers, timeout=10)
            r.encoding = "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            
            # ë¸Œëœë“œë³„ ì„ íƒì
            selector = "li.prod_list" if brand_info["id"] == 1 else "ul.goods_list li.goods_item, li.goods_item"
            items = soup.select(selector)
            
            if not items:
                print(f"  ğŸ›‘ í˜ì´ì§€ {page}: ë! (ì´ {len(new_products)}ê°œ)")
                break
            
            count_in_page = 0
            for item in items:
                p = parse_product(item, category_name, brand_info)
                if p and p['external_id'] > max_external_id:
                    new_products.append(p)
                    count_in_page += 1
            
            print(f"  âœ… í˜ì´ì§€ {page}: {count_in_page}ê°œ (ëˆ„ì  {len(new_products)})")
            time.sleep(0.2)
            
        except Exception as e:
            print(f"  âŒ í˜ì´ì§€ {page}: {e}")
            break
    
    return new_products

def remove_duplicates(products):
    unique = {}
    for p in products:
        key = f"{p['brand_id']}_{p['external_id']}"
        unique[key] = p
    return list(unique.values())

def chunk(lst, size):
    for i in range(0, len(lst), size):
        yield lst[i:i+size]

def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")

    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    total_saved = 0
    
    # âœ… ëª¨ë“  ë¸Œëœë“œ ìˆœì°¨ ì²˜ë¦¬
    for brand_name, brand_info in BRANDS.items():
        print(f"\n{'='*70}")
        print(f"ğŸš€ {brand_name} í¬ë¡¤ë§ ì‹œì‘!")
        print(f"{'='*70}")
        
        brand_saved = 0
        
        for cat in brand_info["categories"]:
            cat_id = cat["id"]
            cat_name = cat["name"]
            
            print(f"\nğŸ“¦ [{brand_name}] {cat_name} (ID: {cat_id})")
            
            # ìµœëŒ€ external_id ì¡°íšŒ
            try:
                last_item = supabase.table("new_products") \
                    .select("external_id") \
                    .eq("brand_id", brand_info["id"]) \
                    .eq("category", cat_name) \
                    .not_.is_("external_id", None) \
                    .order("external_id", desc=True) \
                    .limit(1) \
                    .execute()
                
                max_id = last_item.data[0]['external_id'] if last_item.data else 0
            except:
                max_id = 0

            # ì‹ ìƒí’ˆ í¬ë¡¤ë§
            raw_products = fetch_new_products(supabase, brand_info, cat_id, cat_name, max_id)
            
            if raw_products:
                unique_products = remove_duplicates(raw_products)
                saved_count = 0
                
                for chunk_list in chunk(unique_products, CHUNK_SIZE):
                    try:
                        supabase.table("new_products").insert(chunk_list).execute()
                        saved_count += len(chunk_list)
                    except Exception as e:
                        print(f"ì €ì¥ ì‹¤íŒ¨: {e}")
                        break
                
                print(f"ğŸ’¾ [{brand_name} {cat_name}] {saved_count}ê°œ ì €ì¥!")
                brand_saved += saved_count
                total_saved += saved_count
        
        print(f"âœ… {brand_name} ì™„ë£Œ: {brand_saved}ê°œ")
    
    print(f"\n{'='*70}")
    print(f"ğŸ‰ ì „ì²´ ì™„ë£Œ! CU+GS25 ì´ {total_saved}ê°œ ì‹ ìƒí’ˆ ì €ì¥")
    print(f"{'='*70}")

if __name__ == "__main__":
    main()
