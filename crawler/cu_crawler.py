import os
import time
import re
import requests
from bs4 import BeautifulSoup
from supabase import create_client

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

MAX_SEARCH_PAGES = 100  # ì¶©ë¶„íˆ í¬ê²Œ
CHUNK_SIZE = 50

def parse_product(item):
    """ìƒí’ˆ íŒŒì‹± (gdIdx í¬í•¨)"""
    try:
        name_tag = item.select_one(".name p")
        title = (name_tag.get_text(strip=True) if name_tag else "").strip()
        
        price_tag = item.select_one(".price strong")
        price_text = (price_tag.get_text(strip=True) if price_tag else "0").replace(",", "").replace("ì›", "")
        price = int(price_text) if price_text.isdigit() else 0

        img_tag = item.select_one("img")
        image_url = ""
        if img_tag:
            image_url = img_tag.get("src") or img_tag.get("data-src") or ""
            if image_url.startswith("//"):
                image_url = "https:" + image_url
            elif image_url.startswith("/"):
                image_url = "https://cu.bgfretail.com" + image_url

        badge_tag = item.select_one(".badge")
        promotion_type = badge_tag.get_text(strip=True) if badge_tag else None

        # gdIdx ì¶”ì¶œ (ì¤‘ë³µ ì²´í¬ìš©)
        gdIdx = None
        onclick_div = item.select_one("div[onclick*='view']")
        if onclick_div:
            onclick = onclick_div.get("onclick", "")
            m = re.search(r"view\s*\(\s*(\d+)\s*\)", onclick)
            if m:
                gdIdx = int(m.group(1))
        
        product_url = f"https://cu.bgfretail.com/product/view.do?gdIdx={gdIdx}&category=product" if gdIdx else "https://cu.bgfretail.com/product/view.do?category=product"
        
        if not title:
            return None

        return {
            "title": title,
            "price": price,
            "image_url": image_url,
            "category": "ì•„ì´ìŠ¤í¬ë¦¼",
            "promotion_type": promotion_type,
            "source_url": product_url,
            "is_active": True,
            "brand_id": 1,
            "external_id": gdIdx  # âœ… ìƒí’ˆ ê³ ìœ ë²ˆí˜¸ (ì¤‘ë³µ ì²´í¬ìš©)
        }
    except Exception as e:
        print(f"íŒŒì‹± ì—ëŸ¬: {e}")
        return None

def fetch_new_products(supabase, max_gdIdx):
    """ì‹ ìƒí’ˆë§Œ í¬ë¡¤ë§"""
    new_products = []
    print(f"ğŸ”„ max_gdIdx={max_gdIdx}ë³´ë‹¤ í° ìƒí’ˆ ì°¾ê¸° ì‹œì‘...")
    
    for page in range(1, MAX_SEARCH_PAGES + 1):
        url = "https://cu.bgfretail.com/product/productAjax.do"
        payload = {
            "pageIndex": page, 
            "searchMainCategory": "40",
            "listType": 0,
            "searchCondition": ""
        }
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        }

        try:
            r = requests.post(url, data=payload, headers=headers, timeout=8)
            r.encoding = "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            items = soup.select("li.prod_list")

            if not items:
                print(f"  ğŸ›‘ í˜ì´ì§€ {page}: ë! (ì´ ì‹ ìƒí’ˆ {len(new_products)}ê°œ)")
                break
            
            count_in_page = 0
            for item in items:
                p = parse_product(item)
                if p and p['external_id'] is not None:
                    # âœ… None ì²´í¬ í›„ ë¹„êµ!
                    if p['external_id'] > max_gdIdx:
                        new_products.append(p)
                        count_in_page += 1
            
            if count_in_page > 0:
                print(f"  âœ… í˜ì´ì§€ {page}: ì‹ ìƒí’ˆ {count_in_page}ê°œ (ëˆ„ì  {len(new_products)})")
            else:
                print(f"  PASS í˜ì´ì§€ {page}")
            
            time.sleep(0.1)
            
        except Exception as e:
            print(f"  âŒ í˜ì´ì§€ {page}: {e}")
            break
    
    return new_products

def chunk(lst, size):
    for i in range(0, len(lst), size):
        yield lst[i:i+size]

def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")

    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    # 1. í˜„ì¬ DBì˜ ìµœëŒ€ external_id ì¡°íšŒ (NULL ì œì™¸)
    try:
        last_item = supabase.table("new_products") \
            .select("external_id") \
            .eq("brand_id", 1) \
            .not_.is_("external_id", None) \
            .order("external_id", desc=True) \
            .limit(1) \
            .execute()
        
        max_gdIdx = last_item.data[0]['external_id'] if last_item.data else 0
        print(f"ğŸ“Š í˜„ì¬ DB ë§ˆì§€ë§‰ ìƒí’ˆ ë²ˆí˜¸: {max_gdIdx}")
        
    except Exception as e:
        print(f"DB ì¡°íšŒ ì—ëŸ¬: {e}, ì „ì²´ ìˆ˜ì§‘ ëª¨ë“œë¡œ ì§„í–‰")
        max_gdIdx = 0

    # 2. ì‹ ìƒí’ˆ í¬ë¡¤ë§
    new_products = fetch_new_products(supabase, max_gdIdx)

    # 3. ì €ì¥
    if new_products:
        print(f"\nğŸ’¾ ì‹ ìƒí’ˆ {len(new_products)}ê°œ ì €ì¥ ì¤‘...")
        
        # ìˆœì„œëŒ€ë¡œ ì €ì¥ (ê°€ì¥ ìµœì‹ ì´ ê°€ì¥ ë§ˆì§€ë§‰ì— ì €ì¥ë¨)
        saved_count = 0
        for chunk_list in chunk(new_products, CHUNK_SIZE):
            try:
                supabase.table("new_products").insert(chunk_list).execute()
                saved_count += len(chunk_list)
                print(f"  {saved_count}/{len(new_products)} ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"  ì €ì¥ ì‹¤íŒ¨: {e}")
                break
        
        print(f"ğŸ‰ ì‹ ìƒí’ˆ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
        
        if new_products:
            print(f"   - ìµœì‹  1ìœ„: {new_products[-1]['title']}")
            print(f"   - ìµœì‹  2ìœ„: {new_products[-2]['title']}")
            
    else:
        print("\nâœ¨ ìƒˆë¡œìš´ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
