import os
import time
import re
import json
import requests
from bs4 import BeautifulSoup
from supabase import create_client
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# âš™ï¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
# ==========================================
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

# ==========================================
# ğŸ§  í†µí•© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸° (ìµœì‹  í™•ì •íŒ)
# ==========================================
def get_standard_category(title, raw_category=None):
    # ... (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)
    pass

# ==========================================
# ğŸª 1. CU í¬ë¡¤ë§ (NEW ì´ë¯¸ì§€ ê°ì§€ ë³µêµ¬ / ì¦ë¶„ë§Œ ìˆ˜í–‰)
# ==========================================
def parse_cu_product(item, raw_cat_name):
    # ... (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)
    pass

def crawl_cu(supabase):
    print("\nğŸš€ CU í¬ë¡¤ë§ ì‹œì‘ (ì¦ë¶„ ë°±ì—…)...")

    cu_categories = [
        {"id": "10", "name": "ê°„í¸ì‹ì‚¬"},
        {"id": "30", "name": "ê³¼ìë¥˜"},
        {"id": "40", "name": "ì•„ì´ìŠ¤í¬ë¦¼"},
        {"id": "50", "name": "ì‹í’ˆ"},
        {"id": "60", "name": "ìŒë£Œ"},
        {"id": "70", "name": "ìƒí™œìš©í’ˆ"}
    ]

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://cu.bgfretail.com"
    }

    for cat in cu_categories:
        print(f"ğŸ” CU ì¡°íšŒ: {cat['name']}")

        all_cu_items = []
        for page in range(1, 21):
            try:
                r = requests.post("https://cu.bgfretail.com/product/productAjax.do", 
                                data={"pageIndex": page, "searchMainCategory": cat['id'], "listType": 0},
                                headers=headers, timeout=10)
                r.encoding = 'utf-8'
                soup = BeautifulSoup(r.text, "html.parser")
                items = soup.select("li.prod_list")
                if not items: break

                for item in items:
                    p = parse_cu_product(item, cat['name'])
                    if p: all_cu_items.append(p)
                time.sleep(0.1)
            except: break

        if len(all_cu_items) > 0:
            print(f"   ğŸ’¾ {len(all_cu_items)}ê°œ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì¤‘...")

            # ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
            existing = supabase.table("new_products").select("external_id, title, category").eq("brand_id", 1).execute()
            existing_map = {item["external_id"]: item for item in existing.data}

            for item in all_cu_items:
                ext_id = item["external_id"]
                if ext_id in existing_map:
                    # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ìœ ì§€
                    item["category"] = existing_map[ext_id]["category"]
                    item["title"] = existing_map[ext_id]["title"]

            try:
                unique_items = {p['external_id']: p for p in all_cu_items}.values()
                items_list = list(unique_items)
                for i in range(0, len(items_list), 100):
                    supabase.table("new_products").upsert(
                        items_list[i:i+100], 
                        on_conflict="brand_id,external_id"
                    ).execute()
            except Exception as e: print(f"âŒ CU ì €ì¥ ì‹¤íŒ¨: {e}")

# ==========================================
# ğŸª 2. GS25 í¬ë¡¤ë§ (ì¦ë¶„ ë°±ì—…)
# ==========================================
def get_gs25_token():
    # ... (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)
    pass

def crawl_gs25(supabase):
    print("\nğŸš€ GS25 í¬ë¡¤ë§ ì‹œì‘ (ì¦ë¶„)...")

    session, token = get_gs25_token()
    if not token:
        print("âŒ GS25 í† í° ì‹¤íŒ¨")
        return

    session.headers.update({"Accept": "application/json", "X-Requested-With": "XMLHttpRequest"})

    for p_type in ["ONE_TO_ONE", "TWO_TO_ONE"]:
        print(f"ğŸ” GS25 ì¡°íšŒ: {p_type}")
        all_gs_items = []
        for page in range(1, 20):
            try:
                url = "https://gs25.gsretail.com/gscvs/ko/products/event-goods-search"
                payload = {
                    "CSRFToken": token, "pageNum": str(page), "pageSize": "50", 
                    "parameterList": p_type
                }
                r = session.post(url, data=payload, timeout=10)
                try: data = r.json()
                except: data = json.loads(r.text)
                if isinstance(data, str): data = json.loads(data)

                results = data.get("results", [])
                if not results: break

                for item in results:
                    title = item.get("goodsNm", "").strip()
                    price = int(item.get("price", 0))
                    att_id = item.get("attFileId", "")
                    id_match = re.search(r'(\d+)', att_id)
                    ext_id = int(id_match.group(1)[-18:]) if id_match else int(time.time()*1000)
                    promo_map = {"ONE_TO_ONE": "1+1", "TWO_TO_ONE": "2+1"}

                    all_gs_items.append({
                        "title": title,
                        "price": int(item.get("price", 0)),
                        "image_url": item.get("attFileNm", ""),
                        "category": get_standard_category(title, None),
                        "original_category": None,
                        "promotion_type": promo_map.get(p_type, "í–‰ì‚¬"),
                        "brand_id": 2,
                        "source_url": "http://gs25.gsretail.com/gscvs/ko/products/event-goods",
                        "is_active": True,
                        "external_id": ext_id,
                        "is_new": False
                    })
                time.sleep(0.1)
            except: break

        if len(all_gs_items) > 0:
            print(f"   ğŸ’¾ GS25 {len(all_gs_items)}ê°œ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì¤‘...")

            # ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
            existing = supabase.table("new_products").select("external_id, title, category").eq("brand_id", 2).execute()
            existing_map = {item["external_id"]: item for item in existing.data}

            for item in all_gs_items:
                ext_id = item["external_id"]
                if ext_id in existing_map:
                    # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ìœ ì§€
                    item["category"] = existing_map[ext_id]["category"]
                    item["title"] = existing_map[ext_id]["title"]

            try:
                unique_gs = {p['external_id']: p for p in all_gs_items}.values()
                items_list = list(unique_gs)
                for i in range(0, len(items_list), 100):
                    supabase.table("new_products").upsert(items_list[i:i+100], on_conflict="brand_id,external_id").execute()
            except Exception as e: print(f"âŒ GS25 ì €ì¥ ì‹¤íŒ¨: {e}")

# ==========================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ ì„¤ì • ì˜¤ë¥˜")
        return

    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

    # ğŸ§¹ [ì•ˆì „ì¥ì¹˜] ì“°ë ˆê¸° ë°ì´í„°ë§Œ ì‚­ì œ
    try:
        supabase.table("new_products").delete().or_("promotion_type.eq.ë¤,promotion_type.eq.ë¤ì¦ì •,promotion_type.ilike.%GIFT%,original_category.eq.ì¦‰ì„ì¡°ë¦¬").execute()
    except: pass

    crawl_cu(supabase)
    crawl_gs25(supabase)

    print("\nğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    main()
