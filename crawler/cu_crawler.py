import os
import time
import re
import json
import requests
from bs4 import BeautifulSoup
from supabase import create_client

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

# ==========================================
# ğŸ§  í†µí•© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸° (ì•± íƒ­ ì´ë¦„ê³¼ ì¼ì¹˜)
# ==========================================
def get_standard_category(title, raw_category=None):
    # 1. ìƒí™œìš©í’ˆ (ìµœìš°ì„ )
    if any(k in title for k in [
        'ì¹˜ì•½', 'ì¹«ì†”', 'ê°€ê¸€', 'ê°€ê·¸ë¦°', 'í˜ë¦¬ì˜¤', 'ë©”ë””ì•ˆ', '2080', 'ë¦¬ì¹˜', 'ë´íƒˆ', 'ë§ˆìš°ìŠ¤', 'ì‰ì´ë¹™', 'ë©´ë„ê¸°',
        'ë¬¼í‹°ìŠˆ', 'í‹°ìŠˆ', 'ë§ˆìŠ¤í¬', 'ìƒë¦¬ëŒ€', 'ì¤‘í˜•', 'ëŒ€í˜•', 'ì†Œí˜•', 'ì˜¤ë²„ë‚˜ì´íŠ¸', 'ì…ëŠ”ì˜¤ë²„', 'íŒ¨ë“œ', 'ë¼ì´ë„ˆ', 'íƒí°', 'íŒ¬í‹°',
        'ë¼ì—˜', 'ì˜í”¼', 'í™”ì´íŠ¸', 'ì¢‹ì€ëŠë‚Œ', 'ì‹œí¬ë¦¿ë°ì´', 'ì• ë‹ˆë°ì´', 'ë””ì–´ìŠ¤í‚¨', 'ìˆœìˆ˜í•œë©´',
        'ìƒ´í‘¸', 'ë¦°ìŠ¤', 'íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸', 'í—¤ì–´', 'ì„¸ëŸ¼', 'ë¹„ëˆ„', 'ì—˜ë¼ìŠ¤í‹´', 'ì¼€ë¼ì‹œìŠ¤', 'ì˜¤ê°€ë‹ˆìŠ¤íŠ¸', 'ì˜¨ë”ë°”ë””', 'ë°”ë””ì›Œì‹œ',
        'ë¡œì…˜', 'í•¸ë“œí¬ë¦¼', 'ìˆ˜ë”©ì ¤', 'í´ë Œì§•', 'ì›Œí„°ë§ˆì´ë“œ', 'ì—ì„¼ì…œ', 'ì¡´ìŠ¨ì¦ˆ', 'ì•„ë¹„ë…¸', 'ë‹ˆë² ì•„', 'ë©”ë””í', 'ë¦½ì¼€ì–´', 'ì˜¤ì¼',
        'ì„¸ì œ', 'ë½ìŠ¤', 'ìŠˆê°€ë²„ë¸”', 'ë¬´ê· ë¬´ë•Œ', 'íí', 'í”¼ì§€', 'ê±´ì „ì§€', 'ìŠ¤íƒ€í‚¹', 'ë°´ë“œ', 'ì¼íšŒìš©', 'ì œê±°', 'í´ë¦°í•', 'ìš°ì‚°', 'ì–‘ë§'
    ]):
        return "ìƒí™œìš©í’ˆ"

    # 2. ì‹ì‚¬/ë¼ë©´
    if any(k in title for k in [
        'ë„ì‹œë½', 'ê¹€ë°¥', 'ì£¼ë¨¹ë°¥', 'ìƒŒë“œìœ„ì¹˜', 'í–„ë²„ê±°', 'ë¼ë©´', 'ë©´', 'ìš°ë™', 'êµ­ë°¥', 'ì£½', 'íƒ•', 'ì°Œê°œ', 
        'í–‡ë°˜', 'ì»µë°˜', 'í•«ë°”', 'ì†Œì‹œì§€', 'ë§Œë‘', 'ë‹­ê°€ìŠ´ì‚´', 'ì¹˜í‚¨', 'ìœ¡ê°œì¥', 'ê·¸ë˜ë†€ë¼', 'í†µê³¡ë¬¼ë°¥', 'í¬ë©', 'íŠ€ê¹€', 'ë¸Œë¦¬ë˜', 'íŒŒìŠ¤íƒ€'
    ]):
        return "ì‹ì‚¬/ë¼ë©´"

    # 3. ê³¼ì/ê°„ì‹
    if any(k in title for k in [
        'ìŠ¤ë‚µ', 'ì ¤ë¦¬', 'ì‚¬íƒ•', 'ê»Œ', 'ì´ˆì½”', 'ì¿ í‚¤', 'ì¹©', 'ë¹µ', 'ì¼€ìµ', 'ì•½ê³¼', 'ì–‘ê°±', 'í”„ë ˆì²¼', 'íŒì½˜', 
        'ì•„ëª¬ë“œ', 'ìœ¡í¬', 'ì–´ë¬µ', 'ë§›ë°¤', 'ë§ì°¨ë¹µ', 'í—ˆì‰¬', 'ê·¸ë¦­ìš”ê±°íŠ¸', 'ì˜¤íŒœ', 'í‘¸ë”©', 'ë””ì €íŠ¸', 'í‚·ìº£'
    ]):
        return "ê³¼ì/ê°„ì‹"

    # 4. ì•„ì´ìŠ¤
    if any(k in title for k in ['ì•„ì´ìŠ¤', 'ë°”', 'ì½˜', 'íŒŒì¸íŠ¸', 'í•˜ê²ë‹¤ì¦ˆ', 'ë‚˜ëšœë£¨', 'ì„¤ë ˆì„', 'í´ë¼í¬', 'ìŠ¤í¬ë¥˜', 'ë¼ì§€ë°”', 'ë¹™ìˆ˜', 'ìƒ¤ë² íŠ¸']):
        return "ì•„ì´ìŠ¤"

    # 5. ìŒë£Œ
    if any(k in title for k in [
        'ìš°ìœ ', 'ì»¤í”¼', 'ë¼ë–¼', 'ì•„ë©”ë¦¬ì¹´ë…¸', 'ì½œë¼', 'ì‚¬ì´ë‹¤', 'ì—ì´ë“œ', 'ì£¼ìŠ¤', 'ë³´ë¦¬ì°¨', 'ì˜¥ìˆ˜ìˆ˜ìˆ˜ì—¼ì°¨', 
        'ë¹„íƒ€', 'ë°•ì¹´ìŠ¤', 'ìŒí™”', 'ë‘ìœ ', 'ìš”êµ¬ë¥´íŠ¸', 'ìš”ê±°íŠ¸', 'ë¬¼', 'ì›Œí„°', 'í”„ë¡œí‹´', 'ì½¤ë¶€ì°¨', 'ë“œë§í¬', 'ì´ì˜¨'
    ]):
        return "ìŒë£Œ"

    # 6. ì›ë³¸ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
    if raw_category:
        if raw_category in ["ê°„í¸ì‹ì‚¬", "ì¦‰ì„ì¡°ë¦¬", "ì‹í’ˆ"]: return "ì‹ì‚¬/ë¼ë©´"
        if raw_category == "ê³¼ìë¥˜": return "ê³¼ì/ê°„ì‹"
        if raw_category == "ì•„ì´ìŠ¤í¬ë¦¼": return "ì•„ì´ìŠ¤"
        if raw_category == "ìƒí™œìš©í’ˆ": return "ìƒí™œìš©í’ˆ"
        if raw_category == "ìŒë£Œ": return "ìŒë£Œ"

    return "ê¸°íƒ€"

# ==========================================
# ğŸª 1. CU í¬ë¡¤ë§ (ì‚¬ìš©ì ì›ë³¸ ë¡œì§ ë³µêµ¬: ìµœì‹ ìˆœ ì¦ë¶„)
# ==========================================
def parse_cu_product(item, category_name):
    """CU ìƒí’ˆ íŒŒì‹±"""
    try:
        # 1. ì œëª©
        name_tag = item.select_one(".name p")
        title = (name_tag.get_text(strip=True) if name_tag else "").strip()
        if not title: return None
        
        # 2. ê°€ê²©
        price_tag = item.select_one(".price strong")
        price_text = (price_tag.get_text(strip=True) if price_tag else "0").replace(",", "").replace("ì›", "")
        price = int(price_text) if price_text.isdigit() else 0

        # 3. ì´ë¯¸ì§€
        img_tag = item.select_one("img")
        image_url = ""
        if img_tag:
            image_url = img_tag.get("src") or img_tag.get("data-src") or ""
            if image_url.startswith("//"): image_url = "https:" + image_url
            elif image_url.startswith("/"): image_url = "https://cu.bgfretail.com" + image_url

        # 4. í–‰ì‚¬ ì •ë³´
        badge_tag = item.select_one(".badge")
        promotion_type = badge_tag.get_text(strip=True) if badge_tag else "í–‰ì‚¬"

        # 5. gdIdx ì¶”ì¶œ (ë§í¬ìš©, ì •ë ¬ìš©) - ì‚¬ìš©ìë‹˜ ì›ë³¸ ì •ê·œì‹ ì‚¬ìš©
        gdIdx = None
        onclick_div = item.select_one("div[onclick*='view']")
        if onclick_div:
            onclick = onclick_div.get("onclick", "")
            m = re.search(r"view\s*\(\s*(\d+)\s*\)", onclick)
            if m:
                gdIdx = int(m.group(1))
        
        # gdIdxê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ (ë§í¬ ìƒì„± ë¶ˆê°€)
        if not gdIdx: return None

        # 6. ì‹¤ì œ ë§í¬ ìƒì„±
        product_url = f"https://cu.bgfretail.com/product/view.do?category=product&gdIdx={gdIdx}"
        
        # 7. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (í†µí•© ë¶„ë¥˜ê¸° ì‚¬ìš©)
        std_category = get_standard_category(title, category_name)

        return {
            "title": title,
            "price": price,
            "image_url": image_url,
            "category": std_category,
            "original_category": category_name,
            "promotion_type": promotion_type,
            "source_url": product_url,
            "is_active": True,
            "brand_id": 1,
            "external_id": gdIdx # ì •ë ¬ ë° ì¤‘ë³µ ì²´í¬ìš©
        }
    except Exception as e:
        return None

def run_cu_crawler(supabase):
    print("\n" + "="*50)
    print("ğŸš€ CU í¬ë¡¤ë§ (ìµœì‹ ìˆœ ì¦ë¶„ ì—…ë°ì´íŠ¸) ì‹œì‘")
    print("="*50)
    
    # CU ì¹´í…Œê³ ë¦¬ (ìˆ«ì ì½”ë“œ ì‚¬ìš©)
    CU_CATEGORIES = [
        {"id": "10", "name": "ê°„í¸ì‹ì‚¬"},
        {"id": "20", "name": "ì¦‰ì„ì¡°ë¦¬"},
        {"id": "30", "name": "ê³¼ìë¥˜"},
        {"id": "40", "name": "ì•„ì´ìŠ¤í¬ë¦¼"},
        {"id": "50", "name": "ì‹í’ˆ"},
        {"id": "60", "name": "ìŒë£Œ"},
        {"id": "70", "name": "ìƒí™œìš©í’ˆ"}
    ]
    
    total_added = 0
    
    for cat in CU_CATEGORIES:
        cat_id = cat["id"]
        cat_name = cat["name"]
        
        # 1. DBì—ì„œ ê°€ì¥ ìµœì‹ (í°) external_id ì¡°íšŒ
        try:
            res = supabase.table("new_products") \
                .select("external_id") \
                .eq("brand_id", 1) \
                .eq("original_category", cat_name) \
                .order("external_id", desc=True) \
                .limit(1).execute()
            max_gdIdx = res.data[0]['external_id'] if res.data else 0
        except:
            max_gdIdx = 0
            
        print(f"ğŸ” [{cat_name}] (ê¸°ì¤€ ID: {max_gdIdx}) ê²€ìƒ‰ ì¤‘...")
        
        new_products = []
        
        # ìµœëŒ€ 20í˜ì´ì§€ íƒìƒ‰ (ì‹ ìƒí’ˆì€ ì•í˜ì´ì§€ì— ìˆìŒ)
        for page in range(1, 21):
            url = "https://cu.bgfretail.com/product/productAjax.do"
            payload = {
                "pageIndex": page, 
                "searchMainCategory": cat_id,
                "listType": 1 # 1: ìµœì‹ ìˆœ (ì•„ë§ˆë„)
            }
            headers = {
                "User-Agent": "Mozilla/5.0",
                "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8"
            }

            try:
                r = requests.post(url, data=payload, headers=headers, timeout=10)
                r.encoding = "utf-8"
                soup = BeautifulSoup(r.text, "html.parser")
                items = soup.select("li.prod_list")

                if not items: break
                
                page_count = 0
                for item in items:
                    p = parse_cu_product(item, cat_name)
                    if p:
                        # ì´ë¯¸ DBì— ìˆëŠ” ìµœì‹  IDë³´ë‹¤ í° ê²ƒë§Œ ë‹´ê¸°
                        if max_gdIdx == 0 or p['external_id'] > max_gdIdx:
                            new_products.append(p)
                            page_count += 1
                
                # ì´ë²ˆ í˜ì´ì§€ì— ì‹ ê·œ ìƒí’ˆì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¤‘ë‹¨ (ì´ë¯¸ ìµœì‹ ê¹Œì§€ ë‹¤ ë´¤ë‹¤ëŠ” ëœ»)
                if page_count == 0 and max_gdIdx > 0:
                    break
                
                time.sleep(0.1)
                
            except Exception as e:
                print(f"   âŒ í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
                break
        
        # ì €ì¥ (ì¤‘ë³µ ì œê±° í›„)
        if new_products:
            unique_new = {p['external_id']: p for p in new_products}.values()
            print(f"   ğŸ’¾ {len(unique_new)}ê°œ ì‹ ê·œ ìƒí’ˆ ì €ì¥...")
            
            # 100ê°œì”© ë‚˜ëˆ ì„œ ì €ì¥
            items_list = list(unique_new)
            for i in range(0, len(items_list), 100):
                supabase.table("new_products").insert(items_list[i:i+100]).execute()
            total_added += len(unique_new)
        else:
            print("   âœ¨ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")

    print(f"ğŸ“Š CU ì´ {total_added}ê°œ ì¶”ê°€ë¨")


# ==========================================
# ğŸª 2. GS25 í¬ë¡¤ë§ (HTTPS + í† í° + BS4 íŒŒì‹±)
# ==========================================
def run_gs25_crawler(supabase):
    print("\n" + "="*50)
    print("ğŸš€ GS25 í¬ë¡¤ë§ ì‹œì‘ (ì „ì²´ ê°±ì‹ )")
    print("="*50)

    # 1. ì„¸ì…˜ ì„¤ì • ë° í† í° íšë“
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/json, text/javascript, */*; q=0.01",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://gs25.gsretail.com/gscvs/ko/products/event-goods",
        "X-Requested-With": "XMLHttpRequest"
    })

    token = None
    try:
        # HTTPSë¡œ ì ‘ì†
        url = "https://gs25.gsretail.com/gscvs/ko/products/event-goods"
        r = session.get(url, timeout=15)
        
        # í† í° ì°¾ê¸° (ìš°ì„ ìˆœìœ„ 1: Input íƒœê·¸)
        soup = BeautifulSoup(r.text, "html.parser")
        input_token = soup.find("input", {"name": "CSRFToken"})
        if input_token:
            token = input_token['value']
        
        # í† í° ì°¾ê¸° (ìš°ì„ ìˆœìœ„ 2: ì •ê·œì‹)
        if not token:
            match = re.search(r'CSRFToken\s*[:=]\s*["\']([^"\']+)["\']', r.text)
            if match: token = match.group(1)
            
    except Exception as e:
        print(f"âŒ GS25 ì ‘ì† ì‹¤íŒ¨: {e}")
        return

    if not token:
        print("âŒ GS25 í† í° íšë“ ì‹¤íŒ¨. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return

    # 2. í¬ë¡¤ë§ ì§„í–‰
    all_items = []
    promo_types = ["ONE_TO_ONE", "TWO_TO_ONE", "GIFT"]
    promo_map = {"ONE_TO_ONE": "1+1", "TWO_TO_ONE": "2+1", "GIFT": "ë¤ì¦ì •"}
    search_url = "https://gs25.gsretail.com/gscvs/ko/products/event-goods-search"

    for p_type in promo_types:
        print(f"   ğŸ” GS25 ì¡°íšŒ: {p_type}")
        for page in range(1, 25): # ë„‰ë„‰í•˜ê²Œ 25í˜ì´ì§€
            payload = {
                "CSRFToken": token, 
                "pageNum": str(page), 
                "pageSize": "50", 
                "parameterList": p_type
            }
            
            try:
                r = session.post(search_url, data=payload, timeout=15)
                r.encoding = 'utf-8'
                
                try:
                    data = r.json()
                except:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ ì •ì œ í›„ ì¬ì‹œë„
                    data = json.loads(r.text)
                
                # ê°€ë” ë¬¸ìì—´ë¡œ í•œ ë²ˆ ë” ê°ì‹¸ì§„ ê²½ìš°ê°€ ìˆìŒ
                if isinstance(data, str):
                    data = json.loads(data)
                
                results = data.get("results", [])
                if not results: break
                
                for item in results:
                    title = item.get("goodsNm", "").strip()
                    price = int(item.get("price", 0))
                    
                    # í‚¤ì›Œë“œë¡œ ë¶„ë¥˜
                    std_cat = get_standard_category(title, None)
                    
                    # ID ì¶”ì¶œ
                    att_id = item.get("attFileId", "")
                    id_match = re.search(r'(\d+)', att_id)
                    ext_id = int(id_match.group(1)[-18:]) if id_match else int(time.time() * 1000)
                    
                    all_items.append({
                        "title": title,
                        "price": price,
                        "image_url": item.get("attFileNm", ""),
                        "category": std_cat,
                        "original_category": None, # GSëŠ” ì—†ìŒ
                        "promotion_type": promo_map.get(p_type, "í–‰ì‚¬"),
                        "brand_id": 2,
                        "source_url": "http://gs25.gsretail.com/gscvs/ko/products/event-goods",
                        "is_active": True,
                        "external_id": ext_id
                    })
                
                time.sleep(0.3)
            except Exception as e:
                # print(f"      ì˜¤ë¥˜ ë°œìƒ: {e}")
                break

    # 3. ì €ì¥ (GS25ëŠ” ì „ì²´ ì‚­ì œ í›„ ì¬ë“±ë¡ì´ ê¹”ë”í•¨)
    if len(all_items) > 0:
        print(f"   ğŸ’¾ GS25 ì´ {len(all_items)}ê°œ ë°ì´í„° ê°±ì‹  ì¤‘...")
        try:
            supabase.table("new_products").delete().eq("brand_id", 2).execute()
            
            for i in range(0, len(all_items), 100):
                chunk = all_items[i:i+100]
                supabase.table("new_products").insert(chunk).execute()
            print("   âœ… GS25 ê°±ì‹  ì™„ë£Œ")
        except Exception as e:
            print(f"   âŒ GS25 ì €ì¥ ì‹¤íŒ¨: {e}")
    else:
        print("   ğŸ˜± GS25 ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ==========================================
# ğŸš€ ë©”ì¸
# ==========================================
def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ ì„¤ì • ì˜¤ë¥˜: í™˜ê²½ë³€ìˆ˜ ëˆ„ë½")
        return
    
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    try:
        run_cu_crawler(supabase)
    except Exception as e:
        print(f"âŒ CU ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
    try:
        run_gs25_crawler(supabase)
    except Exception as e:
        print(f"âŒ GS25 ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("\nğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    main()
