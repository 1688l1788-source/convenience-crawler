import os
import time
import re
import requests
from bs4 import BeautifulSoup
from supabase import create_client

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

MAX_PRODUCTS = 50
START_PAGE = 40  # ë„‰ë„‰í•˜ê²Œ 40í˜ì´ì§€ë¶€í„° ê±°ê¾¸ë¡œ íƒìƒ‰

def parse_product(item):
    name_tag = item.select_one(".name p")
    title = (name_tag.get_text(strip=True) if name_tag else "").strip()
    
    price_tag = item.select_one(".price strong")
    price_text = (price_tag.get_text(strip=True) if price_tag else "0").replace(",", "").replace("ì›", "")
    price = int(price_text) if price_text.isdigit() else 0

    img_tag = item.select_one("img")
    image_url = ""
    if img_tag:
        image_url = img_tag.get("src") or img_tag.get("data-src") or ""
        if image_url.startswith("//"):
            image_url = "https:" + image_url
        elif image_url.startswith("/"):
            image_url = "https://cu.bgfretail.com" + image_url

    badge_tag = item.select_one(".badge")
    promotion_type = badge_tag.get_text(strip=True) if badge_tag else None

    product_url = "https://cu.bgfretail.com/product/view.do?category=product"
    onclick_div = item.select_one("div[onclick*='view']")
    if onclick_div:
        onclick = onclick_div.get("onclick", "")
        m = re.search(r"view\s*\(\s*(\d+)\s*\)", onclick)
        if m:
            gdIdx = m.group(1)
            product_url = f"https://cu.bgfretail.com/product/view.do?gdIdx={gdIdx}&category=product"

    if not title:
        return None

    return {
        "title": title,
        "price": price,
        "image_url": image_url,
        "category": "ì•„ì´ìŠ¤í¬ë¦¼",
        "promotion_type": promotion_type,
        "source_url": product_url,
        "is_active": True,
        "brand_id": 1
    }

def crawl_icecream():
    products = []
    
    # âœ… ì¤‘ìš”: ë’¤ì—ì„œë¶€í„° ì•ìœ¼ë¡œ (40 -> 39 -> ... -> 1)
    # ê·¸ë˜ì•¼ ìµœì‹  ìƒí’ˆ(ë§ˆì§€ë§‰ í˜ì´ì§€)ë¶€í„° ê¸ìŒ
    print(f"ğŸ”„ ìµœì‹  ìƒí’ˆì„ ì°¾ê¸° ìœ„í•´ í˜ì´ì§€ {START_PAGE}ë¶€í„° ì—­ìˆœ íƒìƒ‰ ì‹œì‘...")
    
    for page in range(START_PAGE, 0, -1):
        if len(products) >= MAX_PRODUCTS:
            break

        url = "https://cu.bgfretail.com/product/productAjax.do"
        payload = {
            "pageIndex": page,
            "searchMainCategory": "40",
            "listType": 0,
            "searchCondition": "", 
        }
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        }

        try:
            r = requests.post(url, data=payload, headers=headers, timeout=10)
            r.encoding = "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            items = soup.select("li.prod_list")

            if not items:
                # ë°ì´í„°ê°€ ì—†ëŠ” í˜ì´ì§€ëŠ” ê±´ë„ˆëœ€ (ì•„ì§ ë í˜ì´ì§€ ë„ë‹¬ ì „ì¼ ìˆ˜ ìˆìŒ)
                # print(f"  í˜ì´ì§€ {page}: ì—†ìŒ")
                continue

            print(f"  âœ… í˜ì´ì§€ {page}: {len(items)}ê°œ ë°œê²¬ (ìµœì‹ ìˆœ ìˆ˜ì§‘ ì¤‘)")

            # í˜ì´ì§€ ë‚´ì—ì„œëŠ” ìœ„->ì•„ë˜ ìˆœì„œì¸ë°, 
            # í˜ì´ì§€ ìì²´ê°€ ìµœì‹ ì´ë©´ ê·¸ ì•ˆì˜ ìƒí’ˆë“¤ë„ ìµœì‹ ì¼ í™•ë¥  ë†’ìŒ.
            # í•˜ì§€ë§Œ ì •í™•í•œ ìˆœì„œë¥¼ ìœ„í•´ ì¼ë‹¨ ìˆ˜ì§‘í•˜ê³  ë‚˜ì¤‘ì— ì •ë ¬/ì €ì¥í•¨.
            
            # í˜ì´ì§€ ì•ˆì—ì„œë„ ê±°ê¾¸ë¡œ(ì•„ë˜ìª½ì´ ë” ìµœì‹ ì¼ ìˆ˜ë„ ìˆìŒ) ë’¤ì§‘ì–´ì„œ ìˆ˜ì§‘?
            # ë³´í†µ í•œ í˜ì´ì§€ ë‚´ì—ì„œëŠ” ìµœì‹ ->êµ¬í˜•ì¼ ìˆ˜ë„, êµ¬í˜•->ìµœì‹ ì¼ ìˆ˜ë„ ìˆìŒ.
            # ì¼ë‹¨ ê·¸ëŒ€ë¡œ ìˆ˜ì§‘.
            
            # ì—¬ê¸°ì„œ itemsë¥¼ reversed í•´ì•¼ í• ê¹Œ?
            # ë§Œì•½ ì „ì²´ ìˆœì„œê°€ 1(êµ¬) -> 10(ì‹ ) ì´ë¼ë©´,
            # 10í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ ì•„ì´í…œì´ "ê°€ì¥ ìµœì‹ "ì¼ ê°€ëŠ¥ì„± í¼.
            # ë”°ë¼ì„œ itemsë„ reversed í•´ì„œ ìˆ˜ì§‘!
            
            for item in reversed(items):
                if len(products) >= MAX_PRODUCTS: break
                p = parse_product(item)
                if p:
                    products.append(p)
            
            time.sleep(0.2)
            
        except Exception as e:
            print(f"Error on page {page}: {e}")
            
    return products

def chunk(lst, size):
    for i in range(0, len(lst), size):
        yield lst[i:i+size]

def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")

    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

    # 1. ì‚­ì œ
    supabase.table("new_products").delete().eq("brand_id", 1).execute()

    # 2. í¬ë¡¤ë§ (ë’¤ í˜ì´ì§€ë¶€í„° ì—­ìˆœ ìˆ˜ì§‘)
    products = crawl_icecream()

    # 3. ì €ì¥
    # products ë¦¬ìŠ¤íŠ¸: [ê°€ì¥ ìµœì‹ (ì°°ì˜¥ìˆ˜ìˆ˜), ..., ëœ ìµœì‹ ]
    # ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì•± ê²°ê³¼(DESC): idê°€ í´ìˆ˜ë¡ ìµœì‹ ì´ì–´ì•¼ í•¨.
    # ì¦‰, ëœ ìµœì‹  â†’ ë¨¼ì € ì €ì¥(id=1), ê°€ì¥ ìµœì‹  â†’ ë‚˜ì¤‘ì— ì €ì¥(id=50)
    # ë”°ë¼ì„œ productsë¥¼ "ë’¤ì§‘ì–´ì„œ" ì €ì¥í•´ì•¼ í•¨!
    
    products_to_insert = list(reversed(products))

    if products_to_insert:
        try:
            supabase.table("new_products").insert(products_to_insert).execute()
        except Exception:
            for part in chunk(products_to_insert, 10):
                supabase.table("new_products").insert(part).execute()

    print(f"ì™„ë£Œ: í¬ë¡¤ë§ {len(products)}ê°œ / ì €ì¥ {len(products_to_insert)}ê°œ")
    if products_to_insert:
        # ê°€ì¥ ë§ˆì§€ë§‰ì— ì €ì¥ëœ ê²ƒ(=ê°€ì¥ ìµœì‹ , ì•± 1ë“±) ì¶œë ¥ í™•ì¸
        print(f"1ë“± ì˜ˆìƒ ìƒí’ˆ (ì•± ê¸°ì¤€): {products_to_insert[-1]['title']}")

if __name__ == "__main__":
    main()
