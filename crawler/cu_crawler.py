import os
import time
import re
import json
import requests
from bs4 import BeautifulSoup
from supabase import create_client

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

# ==========================================
# ğŸ§  í†µí•© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸°
# ==========================================
def get_standard_category(title, raw_category=None):
    # 1ë‹¨ê³„: CU ì›ë³¸ ì¹´í…Œê³ ë¦¬ê°€ ëª…í™•í•˜ë©´ ìš°ì„  ì ìš©
    if raw_category:
        if raw_category == "ì•„ì´ìŠ¤í¬ë¦¼": return "ì•„ì´ìŠ¤"
        if raw_category == "ìŒë£Œ": return "ìŒë£Œ"
        if raw_category == "ê³¼ìë¥˜": return "ê³¼ì/ê°„ì‹"
        if raw_category == "ìƒí™œìš©í’ˆ": return "ìƒí™œìš©í’ˆ"
        if raw_category in ["ê°„í¸ì‹ì‚¬", "ì¦‰ì„ì¡°ë¦¬"]: return "ì‹ì‚¬/ë¼ë©´"

    # 2ë‹¨ê³„: í‚¤ì›Œë“œ ë¶„ì„ (GS25 ë˜ëŠ” CU 'ì‹í’ˆ' ì¹´í…Œê³ ë¦¬ìš©)
    if any(k in title for k in ['ì¹˜ì•½', 'ì¹«ì†”', 'ê°€ê·¸ë¦°', 'ê°€ê¸€', 'í˜ë¦¬ì˜¤', 'ìƒë¦¬ëŒ€', 'ìƒ´í‘¸', 'ë¦°ìŠ¤', 'ë°”ë””', 'ë¡œì…˜', 'ë©´ë„ê¸°', 'ì„¸ì œ', 'ë§ˆìŠ¤í¬', 'ë¬¼í‹°ìŠˆ', 'ìŠ¤íƒ€í‚¹', 'ë¦½ì¼€ì–´', 'ë‹ˆë² ì•„']):
        return "ìƒí™œìš©í’ˆ"
    if any(k in title for k in ['ë„ì‹œë½', 'ê¹€ë°¥', 'ì£¼ë¨¹ë°¥', 'ìƒŒë“œìœ„ì¹˜', 'í–„ë²„ê±°', 'ë¼ë©´', 'ë©´', 'ìš°ë™', 'êµ­ë°¥', 'íƒ•', 'ì°Œê°œ', 'í–‡ë°˜', 'ì»µë°˜', 'í•«ë°”', 'ì†Œì‹œì§€', 'ë‹­ê°€ìŠ´ì‚´', 'ìœ¡ê°œì¥', 'íŒŒìŠ¤íƒ€', 'ë–¡ë³¶ì´']):
        return "ì‹ì‚¬/ë¼ë©´"
    if any(k in title for k in ['ìŠ¤ë‚µ', 'ì ¤ë¦¬', 'ì‚¬íƒ•', 'ê»Œ', 'ì´ˆì½”', 'ì¿ í‚¤', 'ì¹©', 'ë¹µ', 'ì¼€ìµ', 'ì•½ê³¼', 'ì–‘ê°±', 'ì•„ëª¬ë“œ', 'ìœ¡í¬', 'ì–´ë¬µ', 'ë§›ë°¤', 'ë§ì°¨ë¹µ', 'í—ˆì‰¬', 'ê·¸ë¦­ìš”ê±°íŠ¸', 'í‘¸ë”©', 'ë””ì €íŠ¸', 'í‚·ìº£']):
        return "ê³¼ì/ê°„ì‹"
    if any(k in title for k in ['ì•„ì´ìŠ¤', 'ë°”', 'ì½˜', 'íŒŒì¸íŠ¸', 'í•˜ê²ë‹¤ì¦ˆ', 'ë‚˜ëšœë£¨', 'ì„¤ë ˆì„', 'í´ë¼í¬', 'ìŠ¤í¬ë¥˜', 'ë¼ì§€ë°”', 'ë¹™ìˆ˜', 'ìƒ¤ë² íŠ¸', 'ì°°ì˜¥ìˆ˜ìˆ˜', 'ë¹µë¹ ë ˆ']):
        return "ì•„ì´ìŠ¤"
    if any(k in title for k in ['ìš°ìœ ', 'ì»¤í”¼', 'ë¼ë–¼', 'ì•„ë©”ë¦¬ì¹´ë…¸', 'ì½œë¼', 'ì‚¬ì´ë‹¤', 'ì—ì´ë“œ', 'ì£¼ìŠ¤', 'ë³´ë¦¬ì°¨', 'ë¹„íƒ€', 'ë°•ì¹´ìŠ¤', 'ë‘ìœ ', 'ìš”êµ¬ë¥´íŠ¸', 'ë¬¼', 'ì›Œí„°', 'í”„ë¡œí‹´', 'í•˜ì´ë³¼']):
        return "ìŒë£Œ"

    return "ê¸°íƒ€"

# ==========================================
# ğŸª 1. CU í¬ë¡¤ë§ (ìµœì‹ ìˆœ + í•„í„° ì™„í™”)
# ==========================================
def parse_cu_product(item, raw_cat_name):
    try:
        name_tag = item.select_one(".name p")
        if not name_tag: return None
        title = name_tag.get_text(strip=True)
        
        price_tag = item.select_one(".price strong")
        price = int(price_tag.get_text(strip=True).replace(",", "")) if price_tag else 0
        
        img_tag = item.select_one("img")
        img_src = ""
        if img_tag:
            img_src = img_tag.get("src") or ""
            if img_src.startswith("//"): img_src = "https:" + img_src
            elif img_src.startswith("/"): img_src = "https://cu.bgfretail.com" + img_src

        # âœ… [í•µì‹¬ ìˆ˜ì •] ë°°ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹ ê°œì„ 
        badge_tag = item.select_one(".badge")
        promo = "í–‰ì‚¬" # ê¸°ë³¸ê°’
        if badge_tag:
            # <span> íƒœê·¸ ì•ˆì˜ í…ìŠ¤íŠ¸ë§Œ ê¹”ë”í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
            span = badge_tag.select_one("span")
            if span:
                promo = span.get_text(strip=True)
            else:
                promo = badge_tag.get_text(strip=True)
        
        # [ë””ë²„ê¹…] ì°°ì˜¥ìˆ˜ìˆ˜ ë°œê²¬ ì‹œ ë¡œê·¸ ì¶œë ¥
        if "ì°°ì˜¥ìˆ˜ìˆ˜" in title:
            print(f"   ğŸ‘€ [ë””ë²„ê¹…] '{title}' ë°œê²¬! í–‰ì‚¬íƒœê·¸: '{promo}'")

        # 1+1, 2+1 ì´ ì•„ë‹ˆë”ë¼ë„ ìˆ˜ì§‘í•˜ë„ë¡ ë³€ê²½ (ì‹ ìƒí’ˆ, í• ì¸ ë“±ë„ í¬í•¨í•˜ê³  ì‹¶ìœ¼ì‹  ê²½ìš°)
        # ë§Œì•½ 1+1, 2+1ë§Œ ì›í•˜ì‹œë©´ ì•„ë˜ ì£¼ì„ì„ í‘¸ì„¸ìš”.
        # if promo not in ["1+1", "2+1"]: return None 

        gdIdx = None
        onclick = item.select_one("div[onclick*='view']")
        if onclick:
            m = re.search(r"view\s*\(\s*['\"]?(\d+)['\"]?\s*\)", onclick['onclick'])
            if m: gdIdx = int(m.group(1))
        
        if not gdIdx: return None

        return {
            "title": title,
            "price": price,
            "image_url": img_src,
            "category": get_standard_category(title, raw_cat_name),
            "original_category": raw_cat_name,
            "promotion_type": promo,
            "brand_id": 1,
            "source_url": f"https://cu.bgfretail.com/product/view.do?category=product&gdIdx={gdIdx}",
            "is_active": True,
            "external_id": gdIdx
        }
    except: return None

def crawl_cu(supabase):
    print("\nğŸš€ CU í¬ë¡¤ë§ ì‹œì‘...")
    
    # DB ì´ˆê¸°í™” (ê¹”ë”í•œ ì¬ìˆ˜ì§‘ì„ ìœ„í•´)
    supabase.table("new_products").delete().eq("brand_id", 1).execute()

    cu_categories = [
        {"id": "40", "name": "ì•„ì´ìŠ¤í¬ë¦¼"},
        {"id": "10", "name": "ê°„í¸ì‹ì‚¬"},
        {"id": "20", "name": "ì¦‰ì„ì¡°ë¦¬"},
        {"id": "30", "name": "ê³¼ìë¥˜"},
        {"id": "50", "name": "ì‹í’ˆ"},
        {"id": "60", "name": "ìŒë£Œ"},
        {"id": "70", "name": "ìƒí™œìš©í’ˆ"}
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://cu.bgfretail.com"
    }
    
    total_count = 0
    
    for cat in cu_categories:
        cat_id = cat["id"]
        cat_name = cat["name"]
        print(f"ğŸ” CU [{cat_name}] ìµœì‹ ìˆœ ê²€ìƒ‰ ì¤‘...")
        
        cat_items = []
        for page in range(1, 15):
            try:
                # âœ… listType: 1 (ìµœì‹ ìˆœ)ìœ¼ë¡œ ë³€ê²½
                r = requests.post("https://cu.bgfretail.com/product/productAjax.do", 
                                data={"pageIndex": page, "searchMainCategory": cat_id, "listType": 1},
                                headers=headers, timeout=10)
                soup = BeautifulSoup(r.text, "html.parser")
                items = soup.select("li.prod_list")
                
                if not items: break
                
                for item in items:
                    p = parse_cu_product(item, cat_name)
                    if p:
                        cat_items.append(p)
                
                time.sleep(0.1)
            except Exception as e:
                print(f"   âŒ ì—ëŸ¬: {e}")
                break
        
        if cat_items:
            # ì¤‘ë³µ ì œê±° ë° ì €ì¥
            unique = {p['external_id']: p for p in cat_items}.values()
            chunk_list = list(unique)
            print(f"   ğŸ’¾ {len(chunk_list)}ê°œ ì €ì¥...")
            
            for i in range(0, len(chunk_list), 100):
                supabase.table("new_products").insert(chunk_list[i:i+100]).execute()
            total_count += len(chunk_list)

    print(f"ğŸ“Š CU ì´ {total_count}ê°œ ì™„ë£Œ")

# ==========================================
# ğŸª 2. GS25 í¬ë¡¤ë§ (ìœ ì§€)
# ==========================================
def crawl_gs25(supabase):
    print("\nğŸš€ GS25 í¬ë¡¤ë§ ì‹œì‘...")
    
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0", "Referer": "https://gs25.gsretail.com/"})
    
    token = None
    try:
        r = session.get("https://gs25.gsretail.com/gscvs/ko/products/event-goods")
        soup = BeautifulSoup(r.text, "html.parser")
        token = soup.find("input", {"name": "CSRFToken"})['value']
    except:
        print("âŒ GS25 í† í° ì‹¤íŒ¨")
        return

    session.headers.update({"X-Requested-With": "XMLHttpRequest", "Accept": "application/json, text/javascript, */*; q=0.01"})
    all_gs = []

    for p_type in ["ONE_TO_ONE", "TWO_TO_ONE", "GIFT"]:
        print(f"ğŸ” GS25 ì¡°íšŒ: {p_type}")
        for page in range(1, 20):
            try:
                r = session.post("https://gs25.gsretail.com/gscvs/ko/products/event-goods-search",
                               data={"CSRFToken": token, "pageNum": str(page), "pageSize": "50", "parameterList": p_type})
                data = json.loads(r.text) if not isinstance(r.json(), dict) else r.json()
                results = data.get("results", [])
                if not results: break
                
                for item in results:
                    title = item.get("goodsNm", "").strip()
                    id_match = re.search(r'(\d+)', item.get("attFileId", ""))
                    ext_id = int(id_match.group(1)[-18:]) if id_match else int(time.time()*1000)
                    promo_map = {"ONE_TO_ONE": "1+1", "TWO_TO_ONE": "2+1", "GIFT": "ë¤ì¦ì •"}
                    
                    all_gs.append({
                        "title": title,
                        "price": int(item.get("price", 0)),
                        "image_url": item.get("attFileNm", ""),
                        "category": get_standard_category(title, None),
                        "original_category": None,
                        "promotion_type": promo_map[p_type],
                        "brand_id": 2,
                        "source_url": "http://gs25.gsretail.com/gscvs/ko/products/event-goods",
                        "is_active": True,
                        "external_id": ext_id
                    })
                time.sleep(0.1)
            except: break

    if all_gs:
        print(f"   ğŸ’¾ GS25 {len(all_gs)}ê°œ ê°±ì‹ ...")
        supabase.table("new_products").delete().eq("brand_id", 2).execute()
        for i in range(0, len(all_gs), 100):
            supabase.table("new_products").insert(all_gs[i:i+100]).execute()
        print("ğŸ‰ GS25 ì™„ë£Œ")

def main():
    if not SUPABASE_URL or not SUPABASE_KEY: return
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    crawl_cu(supabase)
    crawl_gs25(supabase)
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    main()
