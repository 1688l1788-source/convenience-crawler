import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
import os
import time

try:
    from supabase import create_client, Client
except ImportError:
    from supabase import create_client

class CUCrawler:
    def __init__(self):
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_SERVICE_KEY')
        
        if not supabase_url or not supabase_key:
            raise Exception("Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            self.supabase = create_client(supabase_url, supabase_key)
        except TypeError:
            self.supabase = create_client(
                supabase_url=supabase_url,
                supabase_key=supabase_key
            )
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        self.brand_id = 1
    
    def crawl(self):
        print("ğŸª CU í¬ë¡¤ë§ ì‹œì‘...")
        
        # ì—¬ëŸ¬ URL ì‹œë„
        urls = [
            "https://cu.bgfretail.com/event/plus.do",  # ê¸°íšì „
            "https://cu.bgfretail.com/product/product.do?category=product&depth2=6&sf=N",  # ì‹ ìƒí’ˆ
            "https://cu.bgfretail.com/product/product.do?category=product",  # ì „ì²´ ìƒí’ˆ
        ]
        
        all_products = []
        
        for url in urls:
            try:
                print(f"\nğŸ” ì‹œë„ ì¤‘: {url}")
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                
                print(f"âœ“ ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                print(f"âœ“ ì½˜í…ì¸  ê¸¸ì´: {len(response.content)} bytes")
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒì ì‹œë„
                selectors = [
                    '.prodListWrap .prod_item',
                    '.prod_list li',
                    '.product-list .product-item',
                    '.list_product li',
                    'ul.prod_list > li',
                    '.prod-list li',
                    'div[class*="prod"] li',
                    'article.prod_item',
                ]
                
                items = []
                for selector in selectors:
                    items = soup.select(selector)
                    if items:
                        print(f"âœ“ ì„ íƒì ì„±ê³µ: {selector}")
                        print(f"ğŸ“¦ ë°œê²¬ëœ ìƒí’ˆ: {len(items)}ê°œ")
                        break
                
                if not items:
                    print(f"âœ— ìƒí’ˆì„ ì°¾ì§€ ëª»í•¨")
                    # HTML êµ¬ì¡° íŒíŠ¸ ì¶œë ¥
                    print("\nğŸ“ HTML êµ¬ì¡° ìƒ˜í”Œ:")
                    print(soup.prettify()[:1000])
                    continue
                
                products = self.parse_items(items, url)
                all_products.extend(products)
                
                if len(all_products) >= 20:
                    break
                    
            except Exception as e:
                print(f"âŒ URL ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"\nğŸ“Š ì´ ìˆ˜ì§‘: {len(all_products)}ê°œ")
        return all_products[:20]  # ìµœëŒ€ 20ê°œ
    
    def parse_items(self, items, base_url):
        """ì œí’ˆ ì•„ì´í…œ íŒŒì‹±"""
        products = []
        
        for idx, item in enumerate(items[:20]):
            try:
                # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒìë¡œ ì‹œë„
                title_selectors = ['.prodName', '.prod_name', '.product-name', 
                                   '.name', 'dt', 'h3', 'h4', '.title', 'strong']
                title = None
                for sel in title_selectors:
                    elem = item.select_one(sel)
                    if elem:
                        title = elem.text.strip()
                        break
                
                if not title:
                    # a íƒœê·¸ì˜ title ì†ì„± í™•ì¸
                    a_tag = item.select_one('a')
                    if a_tag and a_tag.get('title'):
                        title = a_tag.get('title').strip()
                
                if not title or len(title) < 2:
                    continue
                
                # ê°€ê²©
                price_selectors = ['.price', '.prod_price', '.product-price', '.val', 'dd']
                price = None
                for sel in price_selectors:
                    elem = item.select_one(sel)
                    if elem:
                        price_text = elem.text.strip()
                        price_match = re.findall(r'\d+', price_text.replace(',', ''))
                        if price_match:
                            price = int(''.join(price_match))
                            break
                
                # ì´ë¯¸ì§€
                img_elem = item.select_one('img')
                image_url = None
                if img_elem:
                    image_url = img_elem.get('src') or img_elem.get('data-src') or img_elem.get('data-original')
                    if image_url:
                        if not image_url.startswith('http'):
                            if image_url.startswith('//'):
                                image_url = 'https:' + image_url
                            elif image_url.startswith('/'):
                                image_url = 'https://cu.bgfretail.com' + image_url
                            else:
                                image_url = 'https://cu.bgfretail.com/' + image_url
                
                # ë§í¬
                link_elem = item.select_one('a')
                source_url = base_url
                if link_elem:
                    href = link_elem.get('href')
                    if href:
                        if href.startswith('http'):
                            source_url = href
                        elif href.startswith('/'):
                            source_url = 'https://cu.bgfretail.com' + href
                
                product = {
                    'brand_id': self.brand_id,
                    'title': title,
                    'normalized_title': self.normalize_title(title),
                    'price': price,
                    'category': self.categorize(title),
                    'launch_date': datetime.now().date().isoformat(),
                    'image_url': image_url,
                    'source_url': source_url,
                    'is_active': True
                }
                
                products.append(product)
                print(f"  âœ“ {idx+1}. {title[:40]}...")
                
            except Exception as e:
                print(f"  âœ— ìƒí’ˆ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        return products
    
    def normalize_title(self, title):
        normalized = re.sub(r'\s+', ' ', title)
        normalized = re.sub(r'[^\w\sê°€-í£]', '', normalized)
        return normalized.strip().upper()
    
    def categorize(self, title):
        title_lower = title.lower()
        keywords = {
            'ìŒë£Œ': ['ìŒë£Œ', 'ì£¼ìŠ¤', 'ì»¤í”¼', 'ìš°ìœ ', 'ì°¨', 'ì›Œí„°', 'ì‚¬ì´ë‹¤', 'ì½œë¼', 'ì—ì´ë“œ'],
            'ê³¼ì': ['ê³¼ì', 'ì´ˆì½œë¦¿', 'ì‚¬íƒ•', 'ì ¤ë¦¬', 'ì¿ í‚¤', 'ë¹„ìŠ¤í‚·', 'ìŠ¤ë‚µ', 'ì¹©'],
            'ì¦‰ì„ì‹í’ˆ': ['ë„ì‹œë½', 'ê¹€ë°¥', 'ìƒŒë“œìœ„ì¹˜', 'ì‚¼ê°ê¹€ë°¥', 'í•«ë„ê·¸', 'í–„ë²„ê±°', 'ë²„ê±°'],
            'ë¼ë©´': ['ë¼ë©´', 'ì»µë¼ë©´', 'ì§œíŒŒê²Œí‹°', 'ì§œì¥ë©´', 'ë³¶ìŒë©´'],
            'ì•„ì´ìŠ¤í¬ë¦¼': ['ì•„ì´ìŠ¤í¬ë¦¼', 'ë¹™ê³¼', 'ì•„ì´ìŠ¤ë°”', 'ì½˜', 'íŒŒì¸íŠ¸']
        }
        for category, words in keywords.items():
            if any(word in title_lower for word in words):
                return category
        return 'ê¸°íƒ€'
    
    def save_to_db(self, products):
        if not products:
            print("ğŸ’¤ ìˆ˜ì§‘ëœ ì œí’ˆ ì—†ìŒ")
            return 0
        
        print(f"\nğŸ’¾ DB ì €ì¥ ì‹œì‘...")
        
        try:
            thirty_days_ago = (datetime.now() - timedelta(days=30)).date().isoformat()
            existing = self.supabase.table('new_products')\
                .select('normalized_title, launch_date')\
                .eq('brand_id', self.brand_id)\
                .gte('launch_date', thirty_days_ago)\
                .execute()
            
            existing_keys = {
                f"{p['normalized_title']}_{p['launch_date']}" 
                for p in existing.data
            }
            
            new_products = [
                p for p in products 
                if f"{p['normalized_title']}_{p['launch_date']}" not in existing_keys
            ]
            
            if new_products:
                self.supabase.table('new_products').insert(new_products).execute()
                print(f"âœ… {len(new_products)}ê°œ ì‹ ì œí’ˆ ì €ì¥ ì™„ë£Œ")
                for p in new_products[:5]:
                    print(f"  - {p['title'][:40]}")
                if len(new_products) > 5:
                    print(f"  ... ì™¸ {len(new_products)-5}ê°œ")
                return len(new_products)
            else:
                print("â„¹ï¸  ì‹ ê·œ ì œí’ˆ ì—†ìŒ (ëª¨ë‘ ê¸°ì¡´ ì œí’ˆ)")
                return 0
                
        except Exception as e:
            print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return 0

def main():
    print("="*60)
    print("ğŸª í¸ì˜ì  ì‹ ì œí’ˆ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"â° ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    try:
        crawler = CUCrawler()
        products = crawler.crawl()
        new_count = crawler.save_to_db(products)
        print("\n" + "="*60)
        print(f"âœ¨ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ìˆ˜ì§‘: {len(products)}ê°œ | ì‹ ê·œ: {new_count}ê°œ")
        print("="*60)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
