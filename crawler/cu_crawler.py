import requests
from bs4 import BeautifulSoup
from supabase import create_client
import os
import time
import re

# --- ì„¤ì • ---
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

TARGET_CATEGORIES = {'40': 'ì•„ì´ìŠ¤í¬ë¦¼'}
MAX_PAGES = 2
MAX_PRODUCTS = 50

CATEGORY_KEYWORDS = {
    'ì•„ì´ìŠ¤í¬ë¦¼': ['ì•„ì´ìŠ¤', 'ì•„ì´ìŠ¤í¬ë¦¼', 'ì½˜', 'ë°”', 'ë¹™ê³¼', 'ì†Œë¥´ë² ', 'ì ¤ë¼ë˜', 'í•˜ë“œ', 'íˆ¬ê²Œë”'],
}

def crawl_general_products(cat_code, cat_name):
    """ì¼ë°˜ ìƒí’ˆ í¬ë¡¤ë§"""
    print(f"  ğŸ›’ ì¼ë°˜ {cat_name} í¬ë¡¤ë§ ì¤‘...")
    products = []
    
    for page in range(1, MAX_PAGES + 1):
        if len(products) >= MAX_PRODUCTS:
            break
            
        url = "https://cu.bgfretail.com/product/productAjax.do"
        payload = {
            "pageIndex": page,
            "searchMainCategory": cat_code,
            "searchSubCategory": "",
            "listType": 0,
            "searchCondition": "setC",
            "searchUseYn": "N",
            "codeParent": cat_code
        }
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        }

        try:
            response = requests.post(url, data=payload, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select("li.prod_list")

            if not items:
                break

            for item in items:
                if len(products) >= MAX_PRODUCTS: break
                product = parse_product(item, cat_name)
                if product:
                    products.append(product)
            
            time.sleep(0.5)

        except Exception as e:
            print(f"    âŒ ìš”ì²­ ì—ëŸ¬: {e}")
    
    print(f"    âœ… ì¼ë°˜ {len(products)}ê°œ ë°œê²¬")
    return products


def crawl_all_pb_products():
    """PB ìƒí’ˆ ì „ì²´ í¬ë¡¤ë§"""
    print(f"\nğŸª PB ì „ì²´ ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘...")
    products = []
    
    for page in range(1, 10):
        url = "https://cu.bgfretail.com/product/pbAjax.do"
        payload = {
            "pageIndex": page,
            "listType": 0,
            "searchCondition": "setA",
            "searchUseYn": "",
            "gdIdx": "0",
            "searchgubun": "CUG",
            "search1": "",
            "search2": "",
            "searchKeyword": ""
        }
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        }

        try:
            response = requests.post(url, data=payload, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select("li.prod_list")

            if not items:
                break

            for item in items:
                product = parse_product(item, None)
                if product:
                    products.append(product)
            
            time.sleep(0.5)

        except Exception as e:
            print(f"    âŒ PB ìš”ì²­ ì—ëŸ¬: {e}")
    
    print(f"  âœ… PB ì „ì²´ {len(products)}ê°œ í¬ë¡¤ë§ ì™„ë£Œ\n")
    return products


def filter_pb_by_keywords(all_pb_products, category_name):
    """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œë¡œ PB ìƒí’ˆ í•„í„°ë§"""
    keywords = CATEGORY_KEYWORDS.get(category_name, [])
    if not keywords:
        return []
    
    filtered = []
    for product in all_pb_products:
        title = product.get('title', '').lower()
        if any(keyword in title for keyword in keywords):
            # âœ… ìƒˆ ë”•ì…”ë„ˆë¦¬ë¡œ ë³µì‚¬í•˜ê³  category ì„¤ì •
            filtered_product = product.copy()
            filtered_product['category'] = category_name
            filtered.append(filtered_product)
    
    return filtered


def parse_product(item, category_name):
    """ê³µí†µ íŒŒì‹± í•¨ìˆ˜"""
    try:
        name_tag = item.select_one(".name p")
        title = name_tag.text.strip() if name_tag else "ì´ë¦„ì—†ìŒ"

        price_tag = item.select_one(".price strong")
        price_text = price_tag.text.strip().replace(",", "").replace("ì›", "") if price_tag else "0"
        price = int(price_text) if price_text.isdigit() else 0

        img_tag = item.select_one("img")
        image_url = ""
        if img_tag:
            image_url = img_tag.get('src') or img_tag.get('data-src') or ""
            if image_url.startswith('//'):
                image_url = f"https:{image_url}"
            elif image_url.startswith('/'):
                image_url = f"https://cu.bgfretail.com{image_url}"

        badge_tag = item.select_one(".badge")
        promotion_type = badge_tag.text.strip() if badge_tag else None

        product_url = "https://cu.bgfretail.com/product/view.do?category=product"
        onclick_div = item.select_one("div[onclick*='view']")
        if onclick_div:
            onclick = onclick_div.get('onclick', '')
            match = re.search(r"view\s*\(\s*(\d+)\s*\)", onclick)
            if match:
                gdIdx = match.group(1)
                product_url = f"https://cu.bgfretail.com/product/view.do?gdIdx={gdIdx}&category=product"

        return {
            "title": title,
            "price": price,
            "image_url": image_url,
            "category": category_name,
            "promotion_type": promotion_type,
            "source_url": product_url,
            "is_active": True,
            "brand_id": 1
        }
    except Exception as e:
        print(f"    âš ï¸ íŒŒì‹± ì—ëŸ¬: {e}")
        return None


def main():
    print("ğŸš€ CU í¬ë¡¤ëŸ¬ ì‹œì‘ (ì¼ë°˜ + PB í†µí•©)")

    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ ì—ëŸ¬: Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    # 1. PB ìƒí’ˆ ì „ì²´ í¬ë¡¤ë§
    all_pb_products = crawl_all_pb_products()
    
    total_count = 0

    # 2. ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
    for cat_code, cat_name in TARGET_CATEGORIES.items():
        print(f"ğŸ“‚ [{cat_name}] ì²˜ë¦¬ ì‹œì‘...")
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        try:
            supabase.table("new_products").delete().eq("category", cat_name).execute()
            print(f"  ğŸ—‘ï¸  ê¸°ì¡´ {cat_name} ë°ì´í„° ì‚­ì œ")
        except Exception as e:
            print(f"  âš ï¸ ì‚­ì œ ì—ëŸ¬: {e}")
        
        # ì¼ë°˜ ìƒí’ˆ
        general_items = crawl_general_products(cat_code, cat_name)
        
        # PB ìƒí’ˆ í•„í„°ë§
        print(f"  ğŸ” PB {cat_name} í•„í„°ë§ ì¤‘...")
        pb_items = filter_pb_by_keywords(all_pb_products, cat_name)
        print(f"    âœ… PB {len(pb_items)}ê°œ ë°œê²¬")
        
        # âœ… ë””ë²„ê¹…: PB ìƒ˜í”Œ í™•ì¸
        if pb_items:
            print(f"  ğŸ“ PB ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
            for i, p in enumerate(pb_items[:3], 1):
                print(f"    {i}. {p.get('title')} | category={p.get('category')}")
        
        # í•©ì¹˜ê¸°
        all_items = general_items + pb_items
        
        # ì €ì¥
        print(f"  ğŸ’¾ ì €ì¥ ì¤‘... (ì¼ë°˜ {len(general_items)} + PB {len(pb_items)} = {len(all_items)}ê°œ)")
        
        saved_count = 0
        for product in reversed(all_items):
            if not product: continue
            try:
                supabase.table("new_products").insert(product).execute()
                saved_count += 1
            except Exception as e:
                print(f"  âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        total_count += saved_count
        print(f"  âœ… {cat_name} ì™„ë£Œ: {saved_count}ê°œ ì €ì¥\n")

    print(f"ğŸ‰ ì „ì²´ ì™„ë£Œ! ì´ {total_count}ê°œ ìƒí’ˆ ì—…ë°ì´íŠ¸")

if __name__ == "__main__":
    main()
