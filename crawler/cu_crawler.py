import os
import time
import re
import json
import requests
from supabase import create_client

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

# ==========================================
# ğŸ§  í†µí•© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸° (ì•± í•„í„°ë§ìš©)
# ==========================================
def get_standard_category(title, raw_category=None):
    """
    ì œëª©ê³¼ ì›ë³¸ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì•±ì—ì„œ ì‚¬ìš©í•  'í‘œì¤€ ì¹´í…Œê³ ë¦¬'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìš°ì„ ìˆœìœ„: ìƒí™œìš©í’ˆ > ì‹ì‚¬/ë¼ë©´ > ê³¼ì/ê°„ì‹ > ì•„ì´ìŠ¤ > ìŒë£Œ > ì›ë³¸ê¸°ë°˜ ë§¤í•‘
    """
    
    # 1. ìƒí™œìš©í’ˆ (ê°€ì¥ ê°•ë ¥í•œ í•„í„°ë§)
    if any(k in title for k in [
        'ì¹˜ì•½', 'ì¹«ì†”', 'ê°€ê·¸ë¦°', 'ê°€ê¸€', 'í˜ë¦¬ì˜¤', 'ë©”ë””ì•ˆ', '2080', 'ë¦¬ì¹˜', 'ë´íƒˆ', 'ë§ˆìš°ìŠ¤', 'ì‰ì´ë¹™', 'ë©´ë„ê¸°',
        'ë¬¼í‹°ìŠˆ', 'í‹°ìŠˆ', 'ë§ˆìŠ¤í¬', 'ìƒë¦¬ëŒ€', 'ì¤‘í˜•', 'ëŒ€í˜•', 'ì†Œí˜•', 'ì˜¤ë²„ë‚˜ì´íŠ¸', 'ì…ëŠ”ì˜¤ë²„', 'íŒ¨ë“œ', 'ë¼ì´ë„ˆ', 'íƒí°', 'íŒ¬í‹°',
        'ë¼ì—˜', 'ì˜í”¼', 'í™”ì´íŠ¸', 'ì¢‹ì€ëŠë‚Œ', 'ì‹œí¬ë¦¿ë°ì´', 'ì• ë‹ˆë°ì´', 'ë””ì–´ìŠ¤í‚¨', 'ìˆœìˆ˜í•œë©´',
        'ìƒ´í‘¸', 'ë¦°ìŠ¤', 'íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸', 'í—¤ì–´', 'ì„¸ëŸ¼', 'ë¹„ëˆ„', 'ì—˜ë¼ìŠ¤í‹´', 'ì¼€ë¼ì‹œìŠ¤', 'ì˜¤ê°€ë‹ˆìŠ¤íŠ¸', 'ì˜¨ë”ë°”ë””', 'ë°”ë””ì›Œì‹œ',
        'ë¡œì…˜', 'í•¸ë“œí¬ë¦¼', 'ìˆ˜ë”©ì ¤', 'í´ë Œì§•', 'ì›Œí„°ë§ˆì´ë“œ', 'ì—ì„¼ì…œ', 'ì¡´ìŠ¨ì¦ˆ', 'ì•„ë¹„ë…¸', 'ë‹ˆë² ì•„', 'ë©”ë””í', 'ë¦½ì¼€ì–´', 'ì˜¤ì¼',
        'ì„¸ì œ', 'ë½ìŠ¤', 'ìŠˆê°€ë²„ë¸”', 'ë¬´ê· ë¬´ë•Œ', 'íí', 'í”¼ì§€', 'ê±´ì „ì§€', 'ìŠ¤íƒ€í‚¹', 'ë°´ë“œ', 'ì¼íšŒìš©', 'ì œê±°', 'í´ë¦°í•', 'ìš°ì‚°', 'ì–‘ë§'
    ]):
        return "ìƒí™œìš©í’ˆ"

    # 2. ì‹ì‚¬/ë¼ë©´ (ì‹í’ˆ ì¤‘ì—ì„œë„ êµ¬ì²´ì ì¸ ì‹ì‚¬ë¥˜)
    if any(k in title for k in [
        'ë„ì‹œë½', 'ê¹€ë°¥', 'ì£¼ë¨¹ë°¥', 'ìƒŒë“œìœ„ì¹˜', 'í–„ë²„ê±°', 'ë¼ë©´', 'ë©´', 'ìš°ë™', 'êµ­ë°¥', 'ì£½', 'íƒ•', 'ì°Œê°œ', 
        'í–‡ë°˜', 'ì»µë°˜', 'í•«ë°”', 'ì†Œì‹œì§€', 'ë§Œë‘', 'ë‹­ê°€ìŠ´ì‚´', 'ì¹˜í‚¨', 'ìœ¡ê°œì¥', 'ê·¸ë˜ë†€ë¼', 'í†µê³¡ë¬¼ë°¥', 'í¬ë©', 'íŠ€ê¹€', 'ë¸Œë¦¬ë˜', 'íŒŒìŠ¤íƒ€'
    ]):
        return "ì‹ì‚¬/ë¼ë©´"

    # 3. ê³¼ì/ê°„ì‹
    if any(k in title for k in [
        'ìŠ¤ë‚µ', 'ì ¤ë¦¬', 'ì‚¬íƒ•', 'ê»Œ', 'ì´ˆì½”', 'ì¿ í‚¤', 'ì¹©', 'ë¹µ', 'ì¼€ìµ', 'ì•½ê³¼', 'ì–‘ê°±', 'í”„ë ˆì²¼', 'íŒì½˜', 
        'ì•„ëª¬ë“œ', 'ìœ¡í¬', 'ì–´ë¬µ', 'ë§›ë°¤', 'ë§ì°¨ë¹µ', 'í—ˆì‰¬', 'ê·¸ë¦­ìš”ê±°íŠ¸', 'ì˜¤íŒœ', 'í‘¸ë”©', 'ë””ì €íŠ¸', 'í‚·ìº£'
    ]):
        return "ê³¼ì/ê°„ì‹"

    # 4. ì•„ì´ìŠ¤
    if any(k in title for k in ['ì•„ì´ìŠ¤', 'ë°”', 'ì½˜', 'íŒŒì¸íŠ¸', 'í•˜ê²ë‹¤ì¦ˆ', 'ë‚˜ëšœë£¨', 'ì„¤ë ˆì„', 'í´ë¼í¬', 'ìŠ¤í¬ë¥˜', 'ë¼ì§€ë°”', 'ë¹™ìˆ˜', 'ìƒ¤ë² íŠ¸']):
        return "ì•„ì´ìŠ¤"

    # 5. ìŒë£Œ
    if any(k in title for k in [
        'ìš°ìœ ', 'ì»¤í”¼', 'ë¼ë–¼', 'ì•„ë©”ë¦¬ì¹´ë…¸', 'ì½œë¼', 'ì‚¬ì´ë‹¤', 'ì—ì´ë“œ', 'ì£¼ìŠ¤', 'ë³´ë¦¬ì°¨', 'ì˜¥ìˆ˜ìˆ˜ìˆ˜ì—¼ì°¨', 
        'ë¹„íƒ€', 'ë°•ì¹´ìŠ¤', 'ìŒí™”', 'ë‘ìœ ', 'ìš”êµ¬ë¥´íŠ¸', 'ìš”ê±°íŠ¸', 'ë¬¼', 'ì›Œí„°', 'í”„ë¡œí‹´', 'ì½¤ë¶€ì°¨', 'ë“œë§í¬', 'ì´ì˜¨'
    ]):
        return "ìŒë£Œ"

    # 6. í‚¤ì›Œë“œì— ì•ˆ ê±¸ë ¸ì§€ë§Œ CU ì›ë³¸ ì¹´í…Œê³ ë¦¬ê°€ ìˆëŠ” ê²½ìš° ë§¤í•‘
    if raw_category:
        if raw_category in ["ê°„í¸ì‹ì‚¬", "ì¦‰ì„ì¡°ë¦¬", "ì‹í’ˆ"]:
            return "ì‹ì‚¬/ë¼ë©´"
        if raw_category == "ê³¼ìë¥˜":
            return "ê³¼ì/ê°„ì‹"
        if raw_category == "ì•„ì´ìŠ¤í¬ë¦¼":
            return "ì•„ì´ìŠ¤"
        if raw_category == "ìƒí™œìš©í’ˆ":
            return "ìƒí™œìš©í’ˆ"
        if raw_category == "ìŒë£Œ":
            return "ìŒë£Œ"

    return "ê¸°íƒ€"

# ==========================================
# ğŸª 1. CU í¬ë¡¤ë§
# ==========================================
def crawl_cu(supabase):
    print("\nğŸš€ CU í¬ë¡¤ë§ ì‹œì‘...")
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (brand_id=1)
    supabase.table("new_products").delete().eq("brand_id", 1).execute()

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://cu.bgfretail.com/event/product.do",
        "X-Requested-With": "XMLHttpRequest"
    }
    
    # CU ë‚´ë¶€ ì¹´í…Œê³ ë¦¬ ì½”ë“œ (ì›ë³¸ ì´ë¦„ ì €ì¥ìš©)
    cu_categories = {
        "GD_01": "ê°„í¸ì‹ì‚¬",
        "GD_02": "ì¦‰ì„ì¡°ë¦¬",
        "GD_03": "ê³¼ìë¥˜",
        "GD_04": "ì•„ì´ìŠ¤í¬ë¦¼",
        "GD_05": "ì‹í’ˆ",
        "GD_06": "ìŒë£Œ",
        "GD_07": "ìƒí™œìš©í’ˆ"
    }
    
    url = "https://cu.bgfretail.com/event/product.do"
    all_products = []

    for code, raw_cat_name in cu_categories.items():
        print(f"ğŸ” CU ì¡°íšŒ: {raw_cat_name} ({code})")
        
        for page in range(1, 15): 
            payload = {"pageIndex": str(page), "listType": "1", "searchCondition": code, "user_id": ""}
            try:
                r = requests.post(url, data=payload, headers=headers, timeout=10)
                r.encoding = 'utf-8'
                
                # HTML íŒŒì‹± (ì •ê·œì‹)
                titles = re.findall(r'<div class="name">.*?<p>(.*?)</p>.*?</div>', r.text, re.DOTALL)
                prices = re.findall(r'<strong>\s*([0-9,]+)\s*</strong>', r.text)
                images = re.findall(r'<img\s+src="(.*?)"', r.text)
                promos = re.findall(r'class="badge">.*?<span>(.*?)</span>', r.text, re.DOTALL)

                if not titles: break

                for i in range(len(titles)):
                    title = titles[i].strip()
                    # 1. í‘œì¤€ ì¹´í…Œê³ ë¦¬ ê²°ì • (í‚¤ì›Œë“œ ìš°ì„  + ì›ë³¸ ì¹´í…Œê³ ë¦¬ ë³´ì¡°)
                    std_category = get_standard_category(title, raw_cat_name)
                    
                    img = images[i]
                    if not img.startswith('http'): img = "https:" + img

                    all_products.append({
                        "title": title,
                        "price": int(prices[i].replace(',', '')),
                        "image_url": img,
                        "category": std_category,       # ì•± í•„í„°ë§ìš© í‘œì¤€ ì¹´í…Œê³ ë¦¬
                        "original_category": raw_cat_name, # CU ì›ë³¸ ì¹´í…Œê³ ë¦¬ (DB ë³´ì¡´ìš©)
                        "promotion_type": promos[i].strip() if i < len(promos) else "í–‰ì‚¬",
                        "brand_id": 1,
                        "source_url": "https://cu.bgfretail.com/event/product.do",
                        "is_active": True,
                        "external_id": int(time.time() * 1000) + i + (int(code[-2:]) * 10000)
                    })
                time.sleep(0.2)
            except Exception as e:
                print(f"   âŒ ì—ëŸ¬: {e}")
                break

    if all_products:
        print(f"ğŸ’¾ CU {len(all_products)}ê°œ ì €ì¥ ì¤‘...")
        for i in range(0, len(all_products), 100):
            supabase.table("new_products").insert(all_products[i:i+100]).execute()

# ==========================================
# ğŸª 2. GS25 í¬ë¡¤ë§
# ==========================================
def crawl_gs25(supabase):
    print("\nğŸš€ GS25 í¬ë¡¤ë§ ì‹œì‘...")
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (brand_id=2)
    supabase.table("new_products").delete().eq("brand_id", 2).execute()

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "http://gs25.gsretail.com/gscvs/ko/products/event-goods",
        "X-Requested-With": "XMLHttpRequest"
    })
    
    try:
        r = session.get("http://gs25.gsretail.com/gscvs/ko/products/event-goods", timeout=10)
        csrf_token = re.search(r'name="CSRFToken" value="([^"]+)"', r.text).group(1)
    except:
        print("âŒ GS25 í† í° íšë“ ì‹¤íŒ¨")
        return

    promo_types = ["ONE_TO_ONE", "TWO_TO_ONE", "GIFT"]
    url = "http://gs25.gsretail.com/gscvs/ko/products/event-goods-search"
    all_products = []

    for p_type in promo_types:
        print(f"ğŸ” GS25 ì¡°íšŒ: {p_type}")
        for page in range(1, 20):
            payload = {"CSRFToken": csrf_token, "pageNum": str(page), "pageSize": "50", "parameterList": p_type}
            try:
                r = session.post(url, data=payload, timeout=10)
                r.encoding = 'utf-8'
                data = json.loads(r.text) if isinstance(r.json(), str) else r.json()
                results = data.get("results", [])
                
                if not results: break
                
                for item in results:
                    title = item.get("goodsNm", "").strip()
                    price = int(item.get("price", 0))
                    
                    # GS25ëŠ” ì›ë³¸ ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë¯€ë¡œ None ì²˜ë¦¬í•˜ê³  í‚¤ì›Œë“œë¡œ ë¶„ë¥˜
                    std_category = get_standard_category(title, None)
                    
                    # ID ì¶”ì¶œ
                    id_match = re.search(r'(\d+)', item.get("attFileId", ""))
                    ext_id = int(id_match.group(1)[-18:]) if id_match else int(time.time() * 1000)

                    promo_map = {"ONE_TO_ONE": "1+1", "TWO_TO_ONE": "2+1", "GIFT": "ë¤ì¦ì •"}

                    all_products.append({
                        "title": title,
                        "price": price,
                        "image_url": item.get("attFileNm", ""),
                        "category": std_category,       # ì•± í•„í„°ë§ìš© í‘œì¤€ ì¹´í…Œê³ ë¦¬
                        "original_category": None,      # GSëŠ” ì›ë³¸ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—†ìŒ
                        "promotion_type": promo_map.get(p_type, "í–‰ì‚¬"),
                        "brand_id": 2,
                        "source_url": "http://gs25.gsretail.com/gscvs/ko/products/event-goods",
                        "is_active": True,
                        "external_id": ext_id
                    })
                time.sleep(0.3)
            except: break

    if all_products:
        print(f"ğŸ’¾ GS25 {len(all_products)}ê°œ ì €ì¥ ì¤‘...")
        for i in range(0, len(all_products), 100):
            supabase.table("new_products").insert(all_products[i:i+100]).execute()

# ==========================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ ì„¤ì • ì˜¤ë¥˜: í™˜ê²½ë³€ìˆ˜ ëˆ„ë½")
        return
    
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    crawl_cu(supabase)
    crawl_gs25(supabase)
    
    print("\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ")

if __name__ == "__main__":
    main()
